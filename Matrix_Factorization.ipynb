{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset among 'ML-100K' and 'ML-1M'\n",
    "dataset = 'ML-100K'\n",
    "\n",
    "# Data split parameters\n",
    "VALIDATION_USERS_RATINGS_COUNT = 4\n",
    "TEST_USERS_RATINGS_COUNT = 6\n",
    "\n",
    "# Model hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.002\n",
    "REGULARIZATION = 0.05\n",
    "EPOCHS = 10 # TODO - Increase this value to 100\n",
    "\n",
    "# Matrix factorization hyperparameters\n",
    "LATENT_DIM = 50 # Concepts count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "    total = total[total[:,3].argsort()] # Sort by timestamp\n",
    "    \n",
    "    users_ratings_counts = {}\n",
    "    train, validation, test = [], [], []\n",
    "    for user_id, item_id, rating, _ in total[:: -1]:\n",
    "        users_ratings_counts[user_id] = users_ratings_counts.get(user_id, 0) + 1\n",
    "        if users_ratings_counts[user_id] <= TEST_USERS_RATINGS_COUNT:\n",
    "            test.append((user_id - 1, item_id - 1, rating))\n",
    "        elif users_ratings_counts[user_id] <= VALIDATION_USERS_RATINGS_COUNT + TEST_USERS_RATINGS_COUNT:\n",
    "            validation.append((user_id - 1, item_id - 1, rating))\n",
    "        else:\n",
    "            train.append((user_id - 1, item_id - 1, rating))\n",
    "    train, validation, test = np.array(train), np.array(validation), np.array(test)\n",
    "\n",
    "    n_u = np.unique(total[:,0]).size  # num of users\n",
    "    n_m = np.unique(total[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_validation = validation.shape[0]  # num of validation ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of validation ratings: {}'.format(n_validation))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, validation, test\n",
    "\n",
    "def load_data_1m(path='./', delimiter='::'):\n",
    "    data = np.genfromtxt(path+'movielens_1m_dataset.dat', skip_header=0, delimiter=delimiter).astype('int32')\n",
    "    data = data[(-data[:,3]).argsort()]\n",
    "\n",
    "    n_u = np.unique(data[:,0]).size  # num of users\n",
    "    n_m = np.unique(data[:,1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    user_dict = {}\n",
    "    for i, user_id in enumerate(np.unique(data[:,0]).tolist()):\n",
    "        user_dict[user_id] = i\n",
    "    item_dict = {}\n",
    "    for i, item_id in enumerate(np.unique(data[:,1]).tolist()):\n",
    "        item_dict[item_id] = i\n",
    "\n",
    "    idx = np.arange(n_r)\n",
    "\n",
    "    users_ratings_counts = {}\n",
    "    train, validation, test = [], [], []\n",
    "    for i in range(n_r):\n",
    "        user_id = user_dict[data[idx[i], 0]]\n",
    "        item_id = item_dict[data[idx[i], 1]]\n",
    "        rating = data[idx[i], 2]\n",
    "        users_ratings_counts[user_id] = users_ratings_counts.get(user_id, 0) + 1\n",
    "        if users_ratings_counts[user_id] <= TEST_USERS_RATINGS_COUNT:\n",
    "            test.append((user_id - 1, item_id - 1, rating))\n",
    "        elif users_ratings_counts[user_id] <= VALIDATION_USERS_RATINGS_COUNT + TEST_USERS_RATINGS_COUNT:\n",
    "            validation.append((user_id - 1, item_id - 1, rating))\n",
    "        else:\n",
    "            train.append((user_id - 1, item_id - 1, rating))\n",
    "\n",
    "    train, validation, test = np.array(train), np.array(validation), np.array(test)\n",
    "\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_validation = validation.shape[0]  # num of validation ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of ratings: {}'.format(n_r))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of validation ratings: {}'.format(n_validation))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, validation, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 943\n",
      "num of movies: 1682\n",
      "num of training ratings: 90570\n",
      "num of validation ratings: 3772\n",
      "num of test ratings: 5658\n"
     ]
    }
   ],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = 'data'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "\n",
    "# Data Load\n",
    "try:\n",
    "    if dataset == 'ML-100K':\n",
    "        path = data_path + '/MovieLens_100K/'\n",
    "        n_m, n_u, train, validation, test = load_data_100k(path=path, delimiter='\\t')\n",
    "\n",
    "    elif dataset == 'ML-1M':\n",
    "        path = data_path + '/MovieLens_1M/'\n",
    "        n_m, n_u, train, validation, test = load_data_1m(path=path, delimiter='::')\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "except ValueError as e:\n",
    "    print('Error: Unable to load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, avg_ratings, latent_dim=LATENT_DIM, reg=REGULARIZATION):\n",
    "        super().__init__()\n",
    "        self.user_emb = Embedding(num_users, latent_dim,\n",
    "                                  embeddings_regularizer=l2(reg),\n",
    "                                  name=\"user_embedding\")\n",
    "        self.item_emb = Embedding(num_items, latent_dim,\n",
    "                                  embeddings_regularizer=l2(reg),\n",
    "                                  name=\"item_embedding\")\n",
    "        self.user_bias = Embedding(num_users, 1,\n",
    "                                   embeddings_regularizer=l2(reg),\n",
    "                                   name=\"user_bias\")\n",
    "        self.item_bias = Embedding(num_items, 1,\n",
    "                                   embeddings_regularizer=l2(reg),\n",
    "                                   name=\"item_bias\")\n",
    "        self.avg_rating = avg_ratings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user, item = inputs\n",
    "        user_vec = self.user_emb(user)\n",
    "        item_vec = self.item_emb(item)\n",
    "        dot_product = tf.reduce_sum(user_vec * item_vec, axis=1)\n",
    "\n",
    "        bias = (\n",
    "            tf.squeeze(self.user_bias(user)) +\n",
    "            tf.squeeze(self.item_bias(item))\n",
    "        )\n",
    "        return dot_product + self.avg_rating + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users, train_items, train_ratings = train[:,0], train[:,1], train[:,2]\n",
    "validation_users, validation_items, validation_ratings = validation[:,0], validation[:,1], validation[:,2]\n",
    "test_users, test_items, test_ratings = test[:,0], test[:,1], test[:,2]\n",
    "\n",
    "train_avg_rating = np.mean(train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1ms/step - loss: 1.2818 - val_loss: 1.4621\n",
      "Epoch 2/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1ms/step - loss: 1.2731 - val_loss: 1.4649\n",
      "Epoch 3/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 1ms/step - loss: 1.2767 - val_loss: 1.4623\n",
      "Epoch 4/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1ms/step - loss: 1.2552 - val_loss: 1.4654\n",
      "Epoch 5/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1ms/step - loss: 1.2686 - val_loss: 1.4624\n",
      "Epoch 6/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 1ms/step - loss: 1.2679 - val_loss: 1.4625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x163a8c65790>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MatrixFactorization(n_u, n_m, train_avg_rating)\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='mse')\n",
    "model.fit(\n",
    "    [train_users, train_items], train_ratings,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=([validation_users, validation_items], validation_ratings),\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step\n",
      "Test RMSE: 1.1923863887786865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "test_ratings_predicted = model.predict([test_users, test_items])\n",
    "test_ratings_predicted = np.clip(test_ratings_predicted, 1, 5)\n",
    "\n",
    "# get rmse\n",
    "test_rmse = root_mean_squared_error(test_ratings, test_ratings_predicted)\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrix, items_matrix, user_biases, item_biases = model.get_weights()\n",
    "ratings_matrix = np.dot(users_matrix, items_matrix.T) + user_biases + item_biases.T + train_avg_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5353148"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_matrix[10, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.5353148"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([np.array([10]), np.array([15])])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open(path + \"mf_prediction.pickle\", 'wb') as f:\n",
    "    pickle.dump(ratings_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5353148"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path + \"mf_prediction.pickle\", 'rb') as f:\n",
    "    ratings_matrix2 = pickle.load(f)\n",
    "ratings_matrix2[10, 15]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
