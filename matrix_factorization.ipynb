{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset among 'ML-100K' and 'ML-1M'\n",
    "dataset = 'ML-1M'\n",
    "ML_1M_TEST_SIZE = 0.1\n",
    "\n",
    "# Model hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.002\n",
    "REGULARIZATION = 0.05\n",
    "EPOCHS = 100\n",
    "\n",
    "# Matrix factorization hyperparameters\n",
    "LATENT_DIM = 50 # Concepts count\n",
    "\n",
    "# TIMESTAMP\n",
    "DAY_SECONDS_LENGTH = 86400\n",
    "\n",
    "# Session\n",
    "SESSION_TIME_GAP_IN_DAYS = 7 # days\n",
    "SESSION_TIME_GAP_SECONDS = SESSION_TIME_GAP_IN_DAYS * DAY_SECONDS_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, Input\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregularizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l2\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow_server_pb2\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_ops \u001b[38;5;28;01mas\u001b[39;00m cross_device_ops_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_lib\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_utils\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Union\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m values \u001b[38;5;28;01mas\u001b[39;00m value_lib\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backprop_util\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\values.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m struct_pb2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce_util\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:205\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_ctx \u001b[38;5;28;01mas\u001b[39;00m autograph_ctx\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m autograph\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_ops\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:99\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_service_pb2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils_exp\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompress\u001b[39m(element):\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_variable_ops\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m internal\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\__init__.py:28\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ragged Tensors.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis package defines ops for manipulating ragged tensors (`tf.RaggedTensor`),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03mAPI docstring: tensorflow.ragged\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_tensor.py:3149\u001b[0m\n\u001b[0;32m   3144\u001b[0m RaggedOrDense \u001b[38;5;241m=\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[Ragged, core_types\u001b[38;5;241m.\u001b[39mTensorLike]\n\u001b[0;32m   3146\u001b[0m \u001b[38;5;66;03m# RaggedTensor must import ragged_ops to ensure that all dispatched ragged ops\u001b[39;00m\n\u001b[0;32m   3147\u001b[0m \u001b[38;5;66;03m# are registered. Ragged ops import RaggedTensor, so import at bottom of the\u001b[39;00m\n\u001b[0;32m   3148\u001b[0m \u001b[38;5;66;03m# file to avoid a partially-initialized module error.\u001b[39;00m\n\u001b[1;32m-> 3149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_ops\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_ops.py:27\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Import all modules in the `ragged` package that define exported symbols.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mAdditional, import ragged_dispatch (which has the side-effect of registering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03mcircular dependencies.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_array_ops\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_autograph\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_batch_gather_ops\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_array_ops.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sort_ops\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_ragged_shape\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_functional_ops\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_math_ops\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\dynamic_ragged_shape.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extension_type\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor \u001b[38;5;28;01mas\u001b[39;00m tensor_lib\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\framework\\extension_type.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops_stack\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor_ops\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_math_ops\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\ops\\composite_tensor_ops.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Operations for ExtensionTypes (aka Composite Tensors).\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor_variant_pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1476\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1645\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:161\u001b[0m, in \u001b[0;36m_path_isfile\u001b[1;34m(path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:153\u001b[0m, in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "    test_size = len(test)\n",
    "    total = total[total[:,3].argsort()] # Sort by timestamp\n",
    "    \n",
    "    train = total[:-test_size]\n",
    "    test = total[-test_size:]\n",
    "\n",
    "    train_users, train_items = set(train[:, 0]), set(train[:, 1])\n",
    "    test = test[[(test_record[0] in train_users and test_record[1] in train_items) for test_record in test]]\n",
    "\n",
    "    user_id_dict = {}\n",
    "    for i, user_id in enumerate(np.unique(train[:,0]).tolist()):\n",
    "        user_id_dict[user_id] = i\n",
    "    \n",
    "    item_id_dict = {}\n",
    "    for i, item_id in enumerate(np.unique(train[:,1]).tolist()):\n",
    "        item_id_dict[item_id] = i\n",
    "\n",
    "    train = np.array([(user_id_dict[record[0]], item_id_dict[record[1]], record[2], record[3]) for record in train])\n",
    "    test = np.array([(user_id_dict[record[0]], item_id_dict[record[1]], record[2], record[3]) for record in test])\n",
    "\n",
    "    n_u = np.unique(train[:,0]).size  # num of users\n",
    "    n_m = np.unique(train[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, test\n",
    "\n",
    "def load_data_1m(path='./', delimiter='::', test_size=ML_1M_TEST_SIZE):\n",
    "    data = np.genfromtxt(path+'movielens_1m_dataset.dat', skip_header=0, delimiter=delimiter).astype('int32')\n",
    "    data = data[(-data[:,3]).argsort()]\n",
    "\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "    \n",
    "    train, test = [], []\n",
    "    user_id_dict, item_id_dict = {}, {}\n",
    "\n",
    "    for i in range(n_r - 1, -1, -1):\n",
    "        user_id, item_id, rating, timestamp = data[i]\n",
    "        if i < int(test_size * n_r): # test set\n",
    "            if user_id in user_id_dict and item_id in item_id_dict:\n",
    "                test.append((user_id_dict[user_id], item_id_dict[item_id], rating, timestamp))\n",
    "        else: # training set\n",
    "            if user_id not in user_id_dict:\n",
    "                user_id_dict[user_id] = len(user_id_dict)\n",
    "            if item_id not in item_id_dict:\n",
    "                item_id_dict[item_id] = len(item_id_dict)\n",
    "            train.append((user_id_dict[user_id], item_id_dict[item_id], rating, timestamp))\n",
    "    \n",
    "    train, test = np.array(train), np.array(test)\n",
    "\n",
    "    n_u = np.unique(train[:,0]).size  # num of users\n",
    "    n_m = np.unique(train[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of ratings: {}'.format(n_r))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 751\n",
      "num of movies: 1616\n",
      "num of training ratings: 80000\n",
      "num of test ratings: 2863\n"
     ]
    }
   ],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = 'data'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "\n",
    "# Data Load\n",
    "try:\n",
    "    if dataset == 'ML-100K':\n",
    "        path = data_path + '/MovieLens_100K/'\n",
    "        n_m, n_u, train, test = load_data_100k(path=path, delimiter='\\t')\n",
    "\n",
    "    elif dataset == 'ML-1M':\n",
    "        path = data_path + '/MovieLens_1M/'\n",
    "        n_m, n_u, train, test = load_data_1m(path=path, delimiter='::')\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "except ValueError as e:\n",
    "    print('Error: Unable to load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_to_sessions(data: list[tuple[int, int, int, int]]) -> dict[tuple[int, int], list[list[int]]]:\n",
    "    user_sessions_dict = {}\n",
    "    for user_id, item_id, _, timestamp in data:\n",
    "        user_id, item_id, timestamp = int(user_id), int(item_id), int(timestamp)\n",
    "        if user_id not in user_sessions_dict:\n",
    "            user_sessions_dict[user_id] = []\n",
    "        if len(user_sessions_dict[user_id]) == 0 or timestamp - user_sessions_dict[user_id][-1][-1][1] > SESSION_TIME_GAP_SECONDS:\n",
    "            user_sessions_dict[user_id].append([])\n",
    "        user_sessions_dict[user_id][-1].append((item_id, timestamp))\n",
    "    for user_id, user_sessions in user_sessions_dict.items():\n",
    "        for user_session_index, user_session_items_ids in enumerate(user_sessions):\n",
    "            user_sessions_dict[user_id][user_session_index] = [item_id for item_id, _ in user_session_items_ids], [timestamp // DAY_SECONDS_LENGTH for _, timestamp in user_session_items_ids]\n",
    "    user_item_sessions_dict = {}\n",
    "    for user_id, user_sessions in user_sessions_dict.items():\n",
    "        for user_session_items_ids, user_session_items_daystamps in user_sessions:\n",
    "            for item_id in user_session_items_ids:\n",
    "                user_item_sessions_dict[(user_id, item_id)] = tuple(user_session_items_ids), tuple(user_session_items_daystamps)\n",
    "    return user_item_sessions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_dict = divide_to_sessions(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(tf.keras.Model):\n",
    "    def __init__(self, users_count, items_count, average_rating, current_day, latent_dim=LATENT_DIM, regularization_factor=REGULARIZATION):\n",
    "        super().__init__()\n",
    "        self.regularization_factor = regularization_factor\n",
    "        self.user_embeddings = Embedding(users_count, latent_dim,\n",
    "                                  embeddings_regularizer=l2(regularization_factor),\n",
    "                                  name=\"user_embedding\")\n",
    "        self.item_embeddings = Embedding(items_count, latent_dim,\n",
    "                                  embeddings_regularizer=l2(regularization_factor),\n",
    "                                  name=\"item_embedding\")\n",
    "        self.average_rating = average_rating\n",
    "        self.user_biases = Embedding(users_count, 1,\n",
    "                                   embeddings_regularizer=l2(regularization_factor),\n",
    "                                   name=\"user_bias\")\n",
    "        self.item_biases = Embedding(items_count, 1,\n",
    "                                   embeddings_regularizer=l2(regularization_factor),\n",
    "                                   name=\"item_bias\")\n",
    "        self.session_biases = Embedding(len(set([session_items_ids for session_items_ids, _ in sessions_dict.values()])), 1,\n",
    "                                      embeddings_regularizer=l2(regularization_factor),\n",
    "                                      name=\"session_bias\")\n",
    "        self.user_session_decaying_rates = Embedding(users_count, 1,\n",
    "                                   embeddings_regularizer=l2(regularization_factor),\n",
    "                                   name=\"user_session_decaying_rate\")\n",
    "        self.current_day = tf.constant(current_day, dtype=tf.float32)\n",
    "\n",
    "\n",
    "        print(\"finish init\")\n",
    "\n",
    "    def get_session_presentation(self, user_id, session_items_ids, session_items_daystamps):\n",
    "        user_embedding = self.user_embeddings(user_id)\n",
    "        \n",
    "        session_items_ids = tf.boolean_mask(session_items_ids, session_items_ids != -1)\n",
    "        session_items_daystamps = tf.boolean_mask(session_items_daystamps, session_items_daystamps != -1)\n",
    "        session_items_embeddings = self.item_embeddings(session_items_ids)\n",
    "        \n",
    "        session_items_predicts = tf.reduce_sum(user_embedding[:, None, :] * session_items_embeddings, axis=2)\n",
    "        user_decaying_rate = tf.squeeze(self.user_session_decaying_rates(user_id))\n",
    "        user_decaying_rate = tf.nn.relu(user_decaying_rate)\n",
    "        session_items_decaying_factors = tf.exp(-tf.abs(session_items_daystamps - self.current_day)\n",
    "                                                 * user_decaying_rate\n",
    "                                                 )\n",
    "\n",
    "        session_items_scores = session_items_predicts * session_items_decaying_factors\n",
    "        return tf.reduce_mean(session_items_scores, axis=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_id, item_id, session_items_ids, session_items_daystamps = inputs\n",
    "\n",
    "        user_embedding = self.user_embeddings(user_id)\n",
    "        item_embedding = self.item_embeddings(item_id)\n",
    "        raw_prediction = tf.reduce_sum(user_embedding * item_embedding, axis=1)\n",
    "\n",
    "        session_predict = self.get_session_presentation(user_id, session_items_ids, session_items_daystamps)\n",
    "\n",
    "        total_bias = (\n",
    "            tf.squeeze(self.user_biases(user_id)) +\n",
    "            tf.squeeze(self.item_biases(item_id)) +\n",
    "            tf.squeeze(self.session_biases(session_predict))\n",
    "        )\n",
    "\n",
    "        return raw_prediction + self.average_rating + total_bias\n",
    "    \n",
    "    def l2_loss(self, y_true, y_predict, user_id, item_id, session_items_ids, session_items_daystamps):\n",
    "        squared_error = tf.square(y_true - y_predict)\n",
    "        user_embedding, item_embedding = self.user_embeddings(user_id), self.item_embeddings(item_id)\n",
    "        user_embedding_norm = tf.reduce_sum(tf.square(user_embedding), axis=1)\n",
    "        item_embedding_norm = tf.reduce_sum(tf.square(item_embedding), axis=1)\n",
    "        user_bias = tf.squeeze(self.user_biases(user_id))\n",
    "        item_bias = tf.squeeze(self.item_biases(item_id))\n",
    "        session_predict = self.get_session_presentation(user_id, session_items_ids, session_items_daystamps)\n",
    "        session_bias = tf.squeeze(self.session_biases(session_predict))\n",
    "        user_bias_norm = tf.square(user_bias)\n",
    "        item_bias_norm = tf.square(item_bias)\n",
    "        session_bias_norm = tf.square(session_bias)\n",
    "        session_decaying_rate = tf.squeeze(self.user_session_decaying_rates(user_id))\n",
    "        session_decaying_rate_norm = tf.square(session_decaying_rate)\n",
    "\n",
    "        regularized_loss = self.regularization_factor * (user_embedding_norm + item_embedding_norm + user_bias_norm + item_bias_norm + session_bias_norm + session_decaying_rate_norm)\n",
    "        return squared_error + regularized_loss\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        (user_id, item_id, session_items_ids, session_items_daystamps), y_true = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_predict = self((user_id, item_id, session_items_ids, session_items_daystamps), training=True)\n",
    "            loss = self.l2_loss(y_true, y_predict, user_id, item_id, session_items_ids, session_items_daystamps)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users, train_items, train_ratings = train[:,0], train[:,1], np.float32(train[:,2])\n",
    "train_sessions_items = [sessions_dict[(row[0], row[1])][0][: sessions_dict[(row[0], row[1])][0].index(row[1]) + 1] for row in train]\n",
    "train_sessions_daystamps = [sessions_dict[(row[0], row[1])][1][: sessions_dict[(row[0], row[1])][0].index(row[1]) + 1] for row in train]\n",
    "max_session_length = max(len(session_items) for session_items in train_sessions_items)\n",
    "train_sessions_items = np.array([session_items + (-1, ) * (max_session_length - len(session_items)) for session_items in train_sessions_items])\n",
    "train_sessions_daystamps = np.array([session_daystamps + (-1, ) * (max_session_length - len(session_daystamps)) for session_daystamps in train_sessions_daystamps], dtype=np.float32)\n",
    "test_users, test_items, test_ratings = test[:,0], test[:,1], np.float32(test[:,2])\n",
    "\n",
    "train_average_rating = np.mean(train_ratings)\n",
    "test_average_timestamp = np.mean(test[:,3])\n",
    "test_average_daystamp = int(test_average_timestamp // DAY_SECONDS_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish init\n",
      "Epoch 1/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1ms/step - loss: 1.1106\n",
      "Epoch 2/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1ms/step - loss: 0.9833\n",
      "Epoch 3/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1ms/step - loss: 0.9420\n",
      "Epoch 4/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 915us/step - loss: 0.9201\n",
      "Epoch 5/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 912us/step - loss: 0.9069\n",
      "Epoch 6/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 914us/step - loss: 0.8976\n",
      "Epoch 7/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 903us/step - loss: 0.8908\n",
      "Epoch 8/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 966us/step - loss: 0.8857\n",
      "Epoch 9/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 903us/step - loss: 0.8819\n",
      "Epoch 10/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 968us/step - loss: 0.8783\n",
      "Epoch 11/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1ms/step - loss: 0.8756\n",
      "Epoch 12/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.8731\n",
      "Epoch 13/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.8715\n",
      "Epoch 14/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1ms/step - loss: 0.8697\n",
      "Epoch 15/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1ms/step - loss: 0.8681\n",
      "Epoch 16/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1ms/step - loss: 0.8668\n",
      "Epoch 17/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1ms/step - loss: 0.8653\n",
      "Epoch 18/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 941us/step - loss: 0.8643\n",
      "Epoch 19/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 984us/step - loss: 0.8629\n",
      "Epoch 20/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1ms/step - loss: 0.8617\n",
      "Epoch 21/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 973us/step - loss: 0.8608\n",
      "Epoch 22/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 879us/step - loss: 0.8596\n",
      "Epoch 23/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 880us/step - loss: 0.8581\n",
      "Epoch 24/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 909us/step - loss: 0.8569\n",
      "Epoch 25/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 967us/step - loss: 0.8554\n",
      "Epoch 26/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 972us/step - loss: 0.8536\n",
      "Epoch 27/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 917us/step - loss: 0.8518\n",
      "Epoch 28/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 935us/step - loss: 0.8498\n",
      "Epoch 29/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 983us/step - loss: 0.8477\n",
      "Epoch 30/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1ms/step - loss: 0.8452\n",
      "Epoch 31/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.8424\n",
      "Epoch 32/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 990us/step - loss: 0.8397\n",
      "Epoch 33/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1ms/step - loss: 0.8363\n",
      "Epoch 34/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 1ms/step - loss: 0.8331\n",
      "Epoch 35/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 983us/step - loss: 0.8292\n",
      "Epoch 36/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 936us/step - loss: 0.8258\n",
      "Epoch 37/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 956us/step - loss: 0.8218\n",
      "Epoch 38/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 931us/step - loss: 0.8181\n",
      "Epoch 39/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 920us/step - loss: 0.8143\n",
      "Epoch 40/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 914us/step - loss: 0.8103\n",
      "Epoch 41/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 956us/step - loss: 0.8064\n",
      "Epoch 42/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 938us/step - loss: 0.8024\n",
      "Epoch 43/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 961us/step - loss: 0.7985\n",
      "Epoch 44/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 957us/step - loss: 0.7945\n",
      "Epoch 45/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1ms/step - loss: 0.7907\n",
      "Epoch 46/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 983us/step - loss: 0.7869\n",
      "Epoch 47/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1ms/step - loss: 0.7827\n",
      "Epoch 48/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 978us/step - loss: 0.7785\n",
      "Epoch 49/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 976us/step - loss: 0.7747\n",
      "Epoch 50/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 957us/step - loss: 0.7704\n",
      "Epoch 51/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 984us/step - loss: 0.7666\n",
      "Epoch 52/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 956us/step - loss: 0.7625\n",
      "Epoch 53/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 903us/step - loss: 0.7582\n",
      "Epoch 54/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 904us/step - loss: 0.7543\n",
      "Epoch 55/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.7496\n",
      "Epoch 56/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1ms/step - loss: 0.7459\n",
      "Epoch 57/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1ms/step - loss: 0.7415\n",
      "Epoch 58/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.7376\n",
      "Epoch 59/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.7333\n",
      "Epoch 60/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 989us/step - loss: 0.7294\n",
      "Epoch 61/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 997us/step - loss: 0.7253\n",
      "Epoch 62/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 950us/step - loss: 0.7212\n",
      "Epoch 63/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 981us/step - loss: 0.7173\n",
      "Epoch 64/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 967us/step - loss: 0.7134\n",
      "Epoch 65/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 953us/step - loss: 0.7095\n",
      "Epoch 66/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 990us/step - loss: 0.7058\n",
      "Epoch 67/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1ms/step - loss: 0.7022\n",
      "Epoch 68/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1ms/step - loss: 0.6985\n",
      "Epoch 69/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 964us/step - loss: 0.6949\n",
      "Epoch 70/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 965us/step - loss: 0.6914\n",
      "Epoch 71/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 963us/step - loss: 0.6879\n",
      "Epoch 72/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 985us/step - loss: 0.6846\n",
      "Epoch 73/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 967us/step - loss: 0.6812\n",
      "Epoch 74/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 963us/step - loss: 0.6782\n",
      "Epoch 75/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 994us/step - loss: 0.6748\n",
      "Epoch 76/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 979us/step - loss: 0.6720\n",
      "Epoch 77/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 974us/step - loss: 0.6689\n",
      "Epoch 78/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1ms/step - loss: 0.6659\n",
      "Epoch 79/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 1ms/step - loss: 0.6630\n",
      "Epoch 80/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1ms/step - loss: 0.6603\n",
      "Epoch 81/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 1ms/step - loss: 0.6576\n",
      "Epoch 82/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 1ms/step - loss: 0.6550\n",
      "Epoch 83/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1ms/step - loss: 0.6523\n",
      "Epoch 84/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 1ms/step - loss: 0.6497\n",
      "Epoch 85/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 1ms/step - loss: 0.6475\n",
      "Epoch 86/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 1ms/step - loss: 0.6451\n",
      "Epoch 87/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1ms/step - loss: 0.6427\n",
      "Epoch 88/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1ms/step - loss: 0.6406\n",
      "Epoch 89/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1ms/step - loss: 0.6385\n",
      "Epoch 90/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1ms/step - loss: 0.6362\n",
      "Epoch 91/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1ms/step - loss: 0.6342\n",
      "Epoch 92/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1ms/step - loss: 0.6322\n",
      "Epoch 93/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1ms/step - loss: 0.6303\n",
      "Epoch 94/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1ms/step - loss: 0.6284\n",
      "Epoch 95/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1ms/step - loss: 0.6267\n",
      "Epoch 96/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1ms/step - loss: 0.6247\n",
      "Epoch 97/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1ms/step - loss: 0.6231\n",
      "Epoch 98/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1ms/step - loss: 0.6215\n",
      "Epoch 99/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 0.6198\n",
      "Epoch 100/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 1ms/step - loss: 0.6184\n"
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(n_u, n_m, train_average_rating, test_average_daystamp)\n",
    "model.compile(optimizer=SGD(learning_rate=LEARNING_RATE))\n",
    "history = model.fit(\n",
    "    [train_users, train_items, train_sessions_items, train_sessions_daystamps], train_ratings,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(users_matrix, items_matrix, user_biases, item_biases, _, _), average_rating = model.get_weights(), model.average_rating\n",
    "ratings_matrix = np.dot(users_matrix, items_matrix.T) + user_biases + item_biases.T + train_average_rating\n",
    "ratings_matrix = np.clip(ratings_matrix, 1, 5)\n",
    "\n",
    "# Save the model\n",
    "with open(path + f\"mf_prediction_{LATENT_DIM}_dims.pickle\", 'wb') as f:\n",
    "    pickle.dump(ratings_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.9772446155548096\n"
     ]
    }
   ],
   "source": [
    "# Get Test Score\n",
    "# test_ratings_predicted = np.clip(model.predict([test_users, test_items]), 1, 5) # predict using the model\n",
    "test_ratings_predicted = np.array([ratings_matrix[test_user, test_item] for test_user, test_item in zip(test_users, test_items)]) # predict using the matrix\n",
    "\n",
    "# check test rmse\n",
    "try:\n",
    "    test_rmse = root_mean_squared_error(test_ratings, test_ratings_predicted)\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE with 25 latent dims: 0.9772446155548096\n",
      "Test RMSE with 50 latent dims: 0.9810189008712769\n",
      "Test RMSE with 75 latent dims: 0.9727696180343628\n",
      "Test RMSE with 100 latent dims: 0.9691781401634216\n"
     ]
    }
   ],
   "source": [
    "FILE_PREFIX, FILE_SUFFIX = \"mf_prediction_\", f\"_dims.pickle\"\n",
    "LATENT_DIMS = (25, 50, 75, 100)\n",
    "for latent_dim in LATENT_DIMS:\n",
    "    with open(path + FILE_PREFIX + str(latent_dim) + FILE_SUFFIX, 'rb') as f:\n",
    "        ratings_matrix = pickle.load(f)\n",
    "    test_ratings_predicted = np.array([ratings_matrix[test_user, test_item] for test_user, test_item in zip(test_users, test_items)])\n",
    "    test_rmse = root_mean_squared_error(test_ratings, test_ratings_predicted)\n",
    "    print(f\"Test RMSE with {latent_dim} latent dims: {test_rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
