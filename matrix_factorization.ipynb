{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset among 'ML-100K' and 'ML-1M'\n",
    "dataset = 'ML-100K'\n",
    "ML_1M_TEST_SIZE = 0.1\n",
    "\n",
    "# Model hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.002\n",
    "REGULARIZATION = 0.05\n",
    "EPOCHS = 10 # TODO - Increase this value to 100\n",
    "\n",
    "# Matrix factorization hyperparameters\n",
    "LATENT_DIM = 25 # Concepts count\n",
    "\n",
    "# Session\n",
    "SESSION_TIME_GAP_SEC = 604800 # 7 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "    test_size = len(test)\n",
    "    total = total[total[:,3].argsort()] # Sort by timestamp\n",
    "    \n",
    "    train = total[:-test_size]\n",
    "    test = total[-test_size:]\n",
    "\n",
    "    train_users, train_items = set(train[:, 0]), set(train[:, 1])\n",
    "    test = test[[(test_record[0] in train_users and test_record[1] in train_items) for test_record in test]]\n",
    "\n",
    "    n_u = np.unique(train[:,0]).size  # num of users\n",
    "    n_m = np.unique(train[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    user_id_dict = {}\n",
    "    for i, user_id in enumerate(np.unique(train[:,0]).tolist()):\n",
    "        user_id_dict[user_id] = i\n",
    "    \n",
    "    item_id_dict = {}\n",
    "    for i, item_id in enumerate(np.unique(train[:,1]).tolist()):\n",
    "        item_id_dict[item_id] = i\n",
    "\n",
    "    train = np.array([(user_id_dict[record[0]], item_id_dict[record[1]], record[2], record[3]) for record in train])\n",
    "    test = np.array([(user_id_dict[record[0]], item_id_dict[record[1]], record[2], record[3]) for record in test])\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, test\n",
    "\n",
    "def load_data_1m(path='./', delimiter='::', test_size=ML_1M_TEST_SIZE):\n",
    "    data = np.genfromtxt(path+'movielens_1m_dataset.dat', skip_header=0, delimiter=delimiter).astype('int32')\n",
    "    data = data[(-data[:,3]).argsort()]\n",
    "\n",
    "    n_u = np.unique(data[:,0]).size  # num of users\n",
    "    n_m = np.unique(data[:,1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    user_dict = {}\n",
    "    for i, user_id in enumerate(np.unique(data[:,0]).tolist()):\n",
    "        user_dict[user_id] = i\n",
    "    item_dict = {}\n",
    "    for i, item_id in enumerate(np.unique(data[:,1]).tolist()):\n",
    "        item_dict[item_id] = i\n",
    "\n",
    "    idx = np.arange(n_r)\n",
    "    \n",
    "    train, test = [], []\n",
    "    training_users = set()\n",
    "    \n",
    "    for i in range(n_r - 1, -1, -1):\n",
    "        user_id = user_dict[data[idx[i], 0]]\n",
    "        item_id = item_dict[data[idx[i], 1]]\n",
    "        rating = data[idx[i], 2]\n",
    "        timestamp = data[idx[i], 3]\n",
    "        if i < int(test_size * n_r): # test set\n",
    "            if user_id not in training_users:\n",
    "                continue\n",
    "            test.append((user_id - 1, item_id, rating, timestamp))\n",
    "        else: # training set\n",
    "            training_users.add(user_id)\n",
    "            train.append((user_id, item_id, rating, timestamp))\n",
    "\n",
    "    train, test = np.array(train), np.array(test)\n",
    "\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of ratings: {}'.format(n_r))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 751\n",
      "num of movies: 1616\n",
      "num of training ratings: 80000\n",
      "num of test ratings: 2863\n"
     ]
    }
   ],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = 'data'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "\n",
    "# Data Load\n",
    "try:\n",
    "    if dataset == 'ML-100K':\n",
    "        path = data_path + '/MovieLens_100K/'\n",
    "        n_m, n_u, train, test = load_data_100k(path=path, delimiter='\\t')\n",
    "\n",
    "    elif dataset == 'ML-1M':\n",
    "        path = data_path + '/MovieLens_1M/'\n",
    "        n_m, n_u, train, test = load_data_1m(path=path, delimiter='::')\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "except ValueError as e:\n",
    "    print('Error: Unable to load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_to_sessions(data: list[tuple[int, int, int, int]]) -> dict[tuple[int, int], list[list[int]]]:\n",
    "    user_sessions_dict = {}\n",
    "    for user_id, item_id, _, timestamp in data:\n",
    "        user_id, item_id, timestamp = int(user_id), int(item_id), int(timestamp)\n",
    "        if user_id not in user_sessions_dict:\n",
    "            user_sessions_dict[user_id] = []\n",
    "        if len(user_sessions_dict[user_id]) == 0 or timestamp - user_sessions_dict[user_id][-1][-1][1] > SESSION_TIME_GAP_SEC:\n",
    "            user_sessions_dict[user_id].append([])\n",
    "        user_sessions_dict[user_id][-1].append((item_id, timestamp))\n",
    "    for user_id, user_sessions in user_sessions_dict.items():\n",
    "        for i, user_session in enumerate(user_sessions):\n",
    "            user_sessions_dict[user_id][i] = [item_id for item_id, _ in user_session]\n",
    "    user_item_sessions_dict = {}\n",
    "    for user_id, user_sessions in user_sessions_dict.items():\n",
    "        for user_session in user_sessions:\n",
    "            for item_id in user_session:\n",
    "                user_item_sessions_dict[(user_id, item_id)] = tuple(user_session)\n",
    "    return user_item_sessions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_dict = divide_to_sessions(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, avg_ratings, sessions_dict, latent_dim=LATENT_DIM, reg=REGULARIZATION):\n",
    "        super().__init__()\n",
    "        self.reg = reg\n",
    "        self.user_emb = Embedding(num_users, latent_dim,\n",
    "                                  embeddings_regularizer=l2(reg),\n",
    "                                  name=\"user_embedding\")\n",
    "        self.item_emb = Embedding(num_items, latent_dim,\n",
    "                                  embeddings_regularizer=l2(reg),\n",
    "                                  name=\"item_embedding\")\n",
    "        self.avg_rating = avg_ratings\n",
    "        self.user_bias = Embedding(num_users, 1,\n",
    "                                   embeddings_regularizer=l2(reg),\n",
    "                                   name=\"user_bias\")\n",
    "        self.item_bias = Embedding(num_items, 1,\n",
    "                                   embeddings_regularizer=l2(reg),\n",
    "                                   name=\"item_bias\")\n",
    "        self.session_bias = Embedding(len(set(sessions_dict.values())), 1,\n",
    "                                      embeddings_regularizer=l2(reg),\n",
    "                                      name=\"session_bias\")\n",
    "        \n",
    "        keys = [f\"{user}_{item}\" for user, item in sessions_dict.keys()]\n",
    "        sessions = [\",\".join(map(str, session)) for session in sessions_dict.values()]\n",
    "        print(sessions[:5])\n",
    "        keys, sessions = tf.constant(keys), tf.constant(sessions)\n",
    "        print(keys.shape, sessions.shape)\n",
    "\n",
    "        self.sessions_dict = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(keys, sessions),\n",
    "            default_value=\"\"\n",
    ")\n",
    "\n",
    "    # def get_session_predict(self, user, item):\n",
    "    #     key = tf.strings.join([tf.strings.format('{}', user), tf.strings.format('{}', item)], separator=\"_\")\n",
    "    #     session_items = self.sessions_dict.lookup(key)\n",
    "    #     session_items = tf.strings.split(session_items, sep=\",\")\n",
    "    #     session_items = tf.map_fn(lambda x: tf.cast(x, tf.int32), session_items)\n",
    "    #     session_items_embeddings = tf.map_fn(self.item_emb, session_items)\n",
    "    #     items_predicts = tf.map_fn(lambda session_item_vec: tf.reduce_sum(self.user_emb(user) * session_item_vec, axis=1), session_items_embeddings)\n",
    "    #     return tf.reduce_mean(items_predicts, axis=0)\n",
    "\n",
    "    def get_session_predict(self, user, item):\n",
    "        key = tf.strings.join([tf.strings.format('{}', user), tf.strings.format('{}', item)], separator=\"_\")\n",
    "        session_items = self.sessions_dict.lookup(key)  # Get the session string\n",
    "        \n",
    "        session_items = tf.strings.split(session_items, sep=\",\")  # Split into string tensors\n",
    "        session_items = tf.strings.to_number(session_items, out_type=tf.int32)  # Convert to int tensor\n",
    "\n",
    "        session_items_float = tf.cast(session_items, tf.float32)  # Ensure compatibility with embeddings\n",
    "        session_items_embeddings = tf.map_fn(self.item_emb, session_items_float)  # Get embeddings\n",
    "\n",
    "        user_embedding = self.user_emb(user)  # Get user embedding (assumed float32)\n",
    "\n",
    "        items_predicts = tf.map_fn(\n",
    "            lambda session_item_vec: tf.reduce_sum(user_embedding * session_item_vec, axis=1),\n",
    "            session_items_embeddings\n",
    "        )\n",
    "\n",
    "        return tf.reduce_mean(items_predicts, axis=0)  # Final prediction\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user, item = inputs\n",
    "        user_vec = self.user_emb(user)\n",
    "        print(user_vec)\n",
    "        item_vec = self.item_emb(item)\n",
    "        print(item_vec)\n",
    "        dot_product = tf.reduce_sum(user_vec * item_vec, axis=1)\n",
    "        session_predict = self.get_session_predict(user, item)\n",
    "        bias = (\n",
    "            tf.squeeze(self.user_bias(user)) +\n",
    "            tf.squeeze(self.item_bias(item)) +\n",
    "            tf.squeeze(self.session_bias(session_predict))\n",
    "        )\n",
    "        return dot_product + self.avg_rating + bias\n",
    "    \n",
    "    def l2_loss(self, y_true, y_pred, user, item):\n",
    "        squared_error = tf.square(y_true - y_pred)\n",
    "        user_vec, item_vec = self.user_emb(user), self.item_emb(item)\n",
    "        user_vec_norm = tf.reduce_sum(tf.square(user_vec), axis=1)\n",
    "        item_vec_norm = tf.reduce_sum(tf.square(item_vec), axis=1)\n",
    "        user_bias = tf.squeeze(self.user_bias(user))\n",
    "        item_bias = tf.squeeze(self.item_bias(item))\n",
    "        session_bias = tf.squeeze(self.session_bias(self.get_session_predict(user, item)))\n",
    "        user_bias_norm = tf.square(user_bias)\n",
    "        item_bias_norm = tf.square(item_bias)\n",
    "        session_norm = tf.square(session_bias)\n",
    "\n",
    "        reg_loss = self.reg * (user_vec_norm + item_vec_norm + user_bias_norm + item_bias_norm + session_norm)\n",
    "        return squared_error + reg_loss\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        (user, item), y_true = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self((user, item), training=True)\n",
    "            loss = self.l2_loss(y_true, y_pred, user, item)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users, train_items, train_ratings = train[:,0], train[:,1], np.float32(train[:,2])\n",
    "test_users, test_items, test_ratings = test[:,0], test[:,1], np.float32(test[:,2])\n",
    "\n",
    "train_avg_rating = np.mean(train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['254,285,297,184,172,107,768,287,914,116,199,404,1060,175,356,209,180,316,97,11,96', '254,285,297,184,172,107,768,287,914,116,199,404,1060,175,356,209,180,316,97,11,96', '254,285,297,184,172,107,768,287,914,116,199,404,1060,175,356,209,180,316,97,11,96', '254,285,297,184,172,107,768,287,914,116,199,404,1060,175,356,209,180,316,97,11,96', '254,285,297,184,172,107,768,287,914,116,199,404,1060,175,356,209,180,316,97,11,96']\n",
      "(80000,) (80000,)\n",
      "Epoch 1/10\n",
      "Tensor(\"user_embedding_1/GatherV2:0\", shape=(1, 25), dtype=float32)\n",
      "Tensor(\"item_embedding_1/GatherV2:0\", shape=(1, 25), dtype=float32)\n",
      "Tensor(\"matrix_factorization_12_1/user_embedding_1/GatherV2:0\", shape=(1, 25), dtype=float32)\n",
      "Tensor(\"matrix_factorization_12_1/item_embedding_1/GatherV2:0\", shape=(1, 25), dtype=float32)\n",
      "Tensor(\"matrix_factorization_12_1/user_embedding_1/GatherV2:0\", shape=(1, 25), dtype=float32)\n",
      "Tensor(\"matrix_factorization_12_1/item_embedding_1/GatherV2:0\", shape=(1, 25), dtype=float32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node StringToNumber defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\asyncio\\events.py\", line 84, in _run\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\t-yzelinger\\AppData\\Local\\Temp\\ipykernel_28600\\1484275807.py\", line 3, in <module>\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 368, in fit\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 216, in function\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 129, in multi_step_on_iterator\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 110, in one_step_on_data\n\n  File \"C:\\Users\\t-yzelinger\\AppData\\Local\\Temp\\ipykernel_28600\\3907597624.py\", line 98, in train_step\n\n  File \"C:\\Users\\t-yzelinger\\AppData\\Local\\Temp\\ipykernel_28600\\3907597624.py\", line 85, in l2_loss\n\n  File \"C:\\Users\\t-yzelinger\\AppData\\Local\\Temp\\ipykernel_28600\\3907597624.py\", line 47, in get_session_predict\n\nStringToNumberOp could not correctly convert string: \n\t [[{{node StringToNumber}}]] [Op:__inference_multi_step_on_iterator_16137]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MatrixFactorization(n_u, n_m, train_avg_rating, sessions_dict)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39mLEARNING_RATE))\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      4\u001b[0m     [train_users, train_items], train_ratings,\n\u001b[0;32m      5\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m      6\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[0;32m      7\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node StringToNumber defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\asyncio\\events.py\", line 84, in _run\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\t-yzelinger\\AppData\\Local\\Temp\\ipykernel_28600\\1484275807.py\", line 3, in <module>\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 368, in fit\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 216, in function\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 129, in multi_step_on_iterator\n\n  File \"c:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 110, in one_step_on_data\n\n  File \"C:\\Users\\t-yzelinger\\AppData\\Local\\Temp\\ipykernel_28600\\3907597624.py\", line 98, in train_step\n\n  File \"C:\\Users\\t-yzelinger\\AppData\\Local\\Temp\\ipykernel_28600\\3907597624.py\", line 85, in l2_loss\n\n  File \"C:\\Users\\t-yzelinger\\AppData\\Local\\Temp\\ipykernel_28600\\3907597624.py\", line 47, in get_session_predict\n\nStringToNumberOp could not correctly convert string: \n\t [[{{node StringToNumber}}]] [Op:__inference_multi_step_on_iterator_16137]"
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(n_u, n_m, train_avg_rating, sessions_dict)\n",
    "model.compile(optimizer=SGD(learning_rate=LEARNING_RATE))\n",
    "history = model.fit(\n",
    "    [train_users, train_items], train_ratings,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings_predicted = model.predict([test_users, test_items])\n",
    "test_ratings_predicted = np.clip(test_ratings_predicted, 1, 5)\n",
    "\n",
    "# check test rmse\n",
    "test_rmse = root_mean_squared_error(test_ratings, test_ratings_predicted)\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrix, items_matrix, user_biases, item_biases = model.get_weights()\n",
    "ratings_matrix = np.dot(users_matrix, items_matrix.T) + user_biases + item_biases.T + train_avg_rating\n",
    "ratings_matrix = np.clip(ratings_matrix, 1, 5)\n",
    "\n",
    "# Save the model\n",
    "with open(path + \"mf_prediction.pickle\", 'wb') as f:\n",
    "    pickle.dump(ratings_matrix, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
