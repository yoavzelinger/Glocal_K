{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset among 'ML-100K' and 'ML-1M'\n",
    "dataset = 'ML-100K'\n",
    "ML_1M_TEST_SIZE = 0.1\n",
    "\n",
    "# Model hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.002\n",
    "REGULARIZATION = 0.05\n",
    "EPOCHS = 100\n",
    "\n",
    "# Matrix factorization hyperparameters\n",
    "LATENT_DIM = 25 # Concepts count\n",
    "\n",
    "# TIMESTAMP\n",
    "DAY_SECONDS_LENGTH = 86400\n",
    "\n",
    "# Session\n",
    "SESSION_TIME_GAP_IN_DAYS = 7 # days\n",
    "SESSION_TIME_GAP_SECONDS = SESSION_TIME_GAP_IN_DAYS * DAY_SECONDS_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "    test_size = len(test)\n",
    "    total = total[total[:,3].argsort()] # Sort by timestamp\n",
    "    \n",
    "    train = total[:-test_size]\n",
    "    test = total[-test_size:]\n",
    "\n",
    "    train_users, train_items = set(train[:, 0]), set(train[:, 1])\n",
    "    test = test[[(test_record[0] in train_users and test_record[1] in train_items) for test_record in test]]\n",
    "\n",
    "    user_id_dict = {}\n",
    "    for i, user_id in enumerate(np.unique(train[:,0]).tolist()):\n",
    "        user_id_dict[user_id] = i\n",
    "    \n",
    "    item_id_dict = {}\n",
    "    for i, item_id in enumerate(np.unique(train[:,1]).tolist()):\n",
    "        item_id_dict[item_id] = i\n",
    "\n",
    "    train = np.array([(user_id_dict[record[0]], item_id_dict[record[1]], record[2], record[3]) for record in train])\n",
    "    test = np.array([(user_id_dict[record[0]], item_id_dict[record[1]], record[2], record[3]) for record in test])\n",
    "\n",
    "    n_u = np.unique(train[:,0]).size  # num of users\n",
    "    n_m = np.unique(train[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, test\n",
    "\n",
    "def load_data_1m(path='./', delimiter='::', test_size=ML_1M_TEST_SIZE):\n",
    "    data = np.genfromtxt(path+'movielens_1m_dataset.dat', skip_header=0, delimiter=delimiter).astype('int32')\n",
    "    data = data[(-data[:,3]).argsort()]\n",
    "\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "    \n",
    "    train, test = [], []\n",
    "    user_id_dict, item_id_dict = {}, {}\n",
    "\n",
    "    for i in range(n_r - 1, -1, -1):\n",
    "        user_id, item_id, rating, timestamp = data[i]\n",
    "        if i < int(test_size * n_r): # test set\n",
    "            if user_id in user_id_dict and item_id in item_id_dict:\n",
    "                test.append((user_id_dict[user_id], item_id_dict[item_id], rating, timestamp))\n",
    "        else: # training set\n",
    "            if user_id not in user_id_dict:\n",
    "                user_id_dict[user_id] = len(user_id_dict)\n",
    "            if item_id not in item_id_dict:\n",
    "                item_id_dict[item_id] = len(item_id_dict)\n",
    "            train.append((user_id_dict[user_id], item_id_dict[item_id], rating, timestamp))\n",
    "    \n",
    "    train, test = np.array(train), np.array(test)\n",
    "\n",
    "    n_u = np.unique(train[:,0]).size  # num of users\n",
    "    n_m = np.unique(train[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of ratings: {}'.format(n_r))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 751\n",
      "num of movies: 1616\n",
      "num of training ratings: 80000\n",
      "num of test ratings: 2863\n"
     ]
    }
   ],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = 'data'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "\n",
    "# Data Load\n",
    "try:\n",
    "    if dataset == 'ML-100K':\n",
    "        path = data_path + '/MovieLens_100K/'\n",
    "        n_m, n_u, train, test = load_data_100k(path=path, delimiter='\\t')\n",
    "\n",
    "    elif dataset == 'ML-1M':\n",
    "        path = data_path + '/MovieLens_1M/'\n",
    "        n_m, n_u, train, test = load_data_1m(path=path, delimiter='::')\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "except ValueError as e:\n",
    "    print('Error: Unable to load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_to_sessions(data: list[tuple[int, int, int, int]]) -> dict[tuple[int, int], list[list[int]]]:\n",
    "    user_sessions_dict = {}\n",
    "    for user_id, item_id, _, timestamp in data:\n",
    "        user_id, item_id, timestamp = int(user_id), int(item_id), int(timestamp)\n",
    "        if user_id not in user_sessions_dict:\n",
    "            user_sessions_dict[user_id] = []\n",
    "        if len(user_sessions_dict[user_id]) == 0 or timestamp - user_sessions_dict[user_id][-1][-1][1] > SESSION_TIME_GAP_SECONDS:\n",
    "            user_sessions_dict[user_id].append([])\n",
    "        user_sessions_dict[user_id][-1].append((item_id, timestamp))\n",
    "    for user_id, user_sessions in user_sessions_dict.items():\n",
    "        for user_session_index, user_session_items_ids in enumerate(user_sessions):\n",
    "            user_sessions_dict[user_id][user_session_index] = [item_id for item_id, _ in user_session_items_ids], [timestamp // DAY_SECONDS_LENGTH for _, timestamp in user_session_items_ids]\n",
    "    user_item_sessions_dict = {}\n",
    "    for user_id, user_sessions in user_sessions_dict.items():\n",
    "        for user_session_items_ids, user_session_items_daystamps in user_sessions:\n",
    "            for item_id in user_session_items_ids:\n",
    "                user_item_sessions_dict[(user_id, item_id)] = tuple(user_session_items_ids), tuple(user_session_items_daystamps)\n",
    "    return user_item_sessions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_dict = divide_to_sessions(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(tf.keras.Model):\n",
    "    def __init__(self, users_count, items_count, average_rating, current_day, latent_dim=LATENT_DIM, regularization_factor=REGULARIZATION):\n",
    "        super().__init__()\n",
    "        self.regularization_factor = regularization_factor\n",
    "        self.user_embeddings = Embedding(users_count, latent_dim,\n",
    "                                  embeddings_regularizer=l2(regularization_factor),\n",
    "                                  name=\"user_embedding\")\n",
    "        self.item_embeddings = Embedding(items_count, latent_dim,\n",
    "                                  embeddings_regularizer=l2(regularization_factor),\n",
    "                                  name=\"item_embedding\")\n",
    "        self.average_rating = average_rating\n",
    "        self.user_biases = Embedding(users_count, 1,\n",
    "                                   embeddings_regularizer=l2(regularization_factor),\n",
    "                                   name=\"user_bias\")\n",
    "        self.item_biases = Embedding(items_count, 1,\n",
    "                                   embeddings_regularizer=l2(regularization_factor),\n",
    "                                   name=\"item_bias\")\n",
    "        self.session_biases = Embedding(len(set([session_items_ids for session_items_ids, _ in sessions_dict.values()])), 1,\n",
    "                                      embeddings_regularizer=l2(regularization_factor),\n",
    "                                      name=\"session_bias\")\n",
    "        self.user_session_decaying_rates = Embedding(users_count, 1,\n",
    "                                   embeddings_regularizer=l2(regularization_factor),\n",
    "                                   name=\"user_session_decaying_rate\")\n",
    "        self.current_day = tf.constant(current_day, dtype=tf.float32)\n",
    "\n",
    "\n",
    "        print(\"finish init\")\n",
    "\n",
    "    def get_session_presentation(self, user_id, session_items_ids, session_items_daystamps):\n",
    "        user_embedding = self.user_embeddings(user_id)\n",
    "        \n",
    "        session_items_ids = tf.boolean_mask(session_items_ids, session_items_ids != -1)\n",
    "        session_items_daystamps = tf.boolean_mask(session_items_daystamps, session_items_daystamps != -1)\n",
    "        session_items_embeddings = self.item_embeddings(session_items_ids)\n",
    "        \n",
    "        session_items_predicts = tf.reduce_sum(user_embedding[:, None, :] * session_items_embeddings, axis=2)\n",
    "        user_decaying_rate = tf.squeeze(self.user_session_decaying_rates(user_id))\n",
    "        user_decaying_rate = tf.nn.relu(user_decaying_rate)\n",
    "        session_items_decaying_factors = tf.exp(-tf.abs(session_items_daystamps - self.current_day)\n",
    "                                                 * user_decaying_rate\n",
    "                                                 )\n",
    "\n",
    "        session_items_scores = session_items_predicts * session_items_decaying_factors\n",
    "        return tf.reduce_mean(session_items_scores, axis=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_id, item_id, session_items_ids, session_items_daystamps = inputs\n",
    "\n",
    "        user_embedding = self.user_embeddings(user_id)\n",
    "        item_embedding = self.item_embeddings(item_id)\n",
    "        raw_prediction = tf.reduce_sum(user_embedding * item_embedding, axis=1)\n",
    "\n",
    "        session_predict = self.get_session_presentation(user_id, session_items_ids, session_items_daystamps)\n",
    "\n",
    "        total_bias = (\n",
    "            tf.squeeze(self.user_biases(user_id)) +\n",
    "            tf.squeeze(self.item_biases(item_id)) +\n",
    "            tf.squeeze(self.session_biases(session_predict))\n",
    "        )\n",
    "\n",
    "        return raw_prediction + self.average_rating + total_bias\n",
    "    \n",
    "    def l2_loss(self, y_true, y_predict, user_id, item_id, session_items_ids, session_items_daystamps):\n",
    "        squared_error = tf.square(y_true - y_predict)\n",
    "        user_embedding, item_embedding = self.user_embeddings(user_id), self.item_embeddings(item_id)\n",
    "        user_embedding_norm = tf.reduce_sum(tf.square(user_embedding), axis=1)\n",
    "        item_embedding_norm = tf.reduce_sum(tf.square(item_embedding), axis=1)\n",
    "        user_bias = tf.squeeze(self.user_biases(user_id))\n",
    "        item_bias = tf.squeeze(self.item_biases(item_id))\n",
    "        session_predict = self.get_session_presentation(user_id, session_items_ids, session_items_daystamps)\n",
    "        session_bias = tf.squeeze(self.session_biases(session_predict))\n",
    "        user_bias_norm = tf.square(user_bias)\n",
    "        item_bias_norm = tf.square(item_bias)\n",
    "        session_bias_norm = tf.square(session_bias)\n",
    "        session_decaying_rate = tf.squeeze(self.user_session_decaying_rates(user_id))\n",
    "        session_decaying_rate_norm = tf.square(session_decaying_rate)\n",
    "\n",
    "        regularized_loss = self.regularization_factor * (user_embedding_norm + item_embedding_norm + user_bias_norm + item_bias_norm + session_bias_norm + session_decaying_rate_norm)\n",
    "        return squared_error + regularized_loss\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        (user_id, item_id, session_items_ids, session_items_daystamps), y_true = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_predict = self((user_id, item_id, session_items_ids, session_items_daystamps), training=True)\n",
    "            loss = self.l2_loss(y_true, y_predict, user_id, item_id, session_items_ids, session_items_daystamps)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users, train_items, train_ratings = train[:,0], train[:,1], np.float32(train[:,2])\n",
    "train_sessions_items = [sessions_dict[(row[0], row[1])][0][: sessions_dict[(row[0], row[1])][0].index(row[1]) + 1] for row in train]\n",
    "train_sessions_daystamps = [sessions_dict[(row[0], row[1])][1][: sessions_dict[(row[0], row[1])][0].index(row[1]) + 1] for row in train]\n",
    "max_session_length = max(len(session_items) for session_items in train_sessions_items)\n",
    "train_sessions_items = np.array([session_items + (-1, ) * (max_session_length - len(session_items)) for session_items in train_sessions_items])\n",
    "train_sessions_daystamps = np.array([session_daystamps + (-1, ) * (max_session_length - len(session_daystamps)) for session_daystamps in train_sessions_daystamps], dtype=np.float32)\n",
    "test_users, test_items, test_ratings = test[:,0], test[:,1], np.float32(test[:,2])\n",
    "\n",
    "train_average_rating = np.mean(train_ratings)\n",
    "test_average_timestamp = np.mean(test[:,3])\n",
    "test_average_daystamp = int(test_average_timestamp // DAY_SECONDS_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish init\n",
      "Epoch 1/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 955us/step - loss: 1.1107\n",
      "Epoch 2/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 989us/step - loss: 0.9839\n",
      "Epoch 3/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 955us/step - loss: 0.9423\n",
      "Epoch 4/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1ms/step - loss: 0.9206\n",
      "Epoch 5/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1ms/step - loss: 0.9072\n",
      "Epoch 6/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1ms/step - loss: 0.8980\n",
      "Epoch 7/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 977us/step - loss: 0.8916\n",
      "Epoch 8/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 964us/step - loss: 0.8863\n",
      "Epoch 9/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 967us/step - loss: 0.8820\n",
      "Epoch 10/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 993us/step - loss: 0.8787\n"
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(n_u, n_m, train_average_rating, test_average_daystamp)\n",
    "model.compile(optimizer=SGD(learning_rate=LEARNING_RATE))\n",
    "history = model.fit(\n",
    "    [train_users, train_items, train_sessions_items, train_sessions_daystamps], train_ratings,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(users_matrix, items_matrix, user_biases, item_biases, _, _), average_rating = model.get_weights(), model.average_rating\n",
    "ratings_matrix = np.dot(users_matrix, items_matrix.T) + user_biases + item_biases.T + train_average_rating\n",
    "ratings_matrix = np.clip(ratings_matrix, 1, 5)\n",
    "\n",
    "# Save the model\n",
    "with open(path + f\"mf_prediction_{LATENT_DIM}_dims.pickle\", 'wb') as f:\n",
    "    pickle.dump(ratings_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.0089982748031616\n"
     ]
    }
   ],
   "source": [
    "# Get Test Score\n",
    "# test_ratings_predicted = np.clip(model.predict([test_users, test_items]), 1, 5) # predict using the model\n",
    "test_ratings_predicted = np.array([ratings_matrix[test_user, test_item] for test_user, test_item in zip(test_users, test_items)]) # predict using the matrix\n",
    "\n",
    "# check test rmse\n",
    "try:\n",
    "    test_rmse = root_mean_squared_error(test_ratings, test_ratings_predicted)\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
