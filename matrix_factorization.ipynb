{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset among 'ML-100K' and 'ML-1M'\n",
    "dataset = 'ML-100K'\n",
    "ML_1M_TEST_SIZE = 0.1\n",
    "\n",
    "# Model hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.002\n",
    "REGULARIZATION = 0.05\n",
    "EPOCHS = 100\n",
    "\n",
    "# Matrix factorization hyperparameters\n",
    "LATENT_DIM = 25 # Concepts count\n",
    "\n",
    "# TIMESTAMP\n",
    "DAY_SECONDS_LENGTH = 86400\n",
    "\n",
    "# Session\n",
    "SESSION_TIME_GAP_IN_DAYS = 7 # days\n",
    "SESSION_TIME_GAP_SECONDS = SESSION_TIME_GAP_IN_DAYS * DAY_SECONDS_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "    test_size = len(test)\n",
    "    total = total[total[:,3].argsort()] # Sort by timestamp\n",
    "    \n",
    "    train = total[:-test_size]\n",
    "    test = total[-test_size:]\n",
    "\n",
    "    train_users, train_items = set(train[:, 0]), set(train[:, 1])\n",
    "    test = test[[(test_record[0] in train_users and test_record[1] in train_items) for test_record in test]]\n",
    "\n",
    "    user_id_dict = {}\n",
    "    for i, user_id in enumerate(np.unique(train[:,0]).tolist()):\n",
    "        user_id_dict[user_id] = i\n",
    "    \n",
    "    item_id_dict = {}\n",
    "    for i, item_id in enumerate(np.unique(train[:,1]).tolist()):\n",
    "        item_id_dict[item_id] = i\n",
    "\n",
    "    train = np.array([(user_id_dict[record[0]], item_id_dict[record[1]], record[2], record[3]) for record in train])\n",
    "    test = np.array([(user_id_dict[record[0]], item_id_dict[record[1]], record[2], record[3]) for record in test])\n",
    "\n",
    "    n_u = np.unique(train[:,0]).size  # num of users\n",
    "    n_m = np.unique(train[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, test\n",
    "\n",
    "def load_data_1m(path='./', delimiter='::', test_size=ML_1M_TEST_SIZE):\n",
    "    data = np.genfromtxt(path+'movielens_1m_dataset.dat', skip_header=0, delimiter=delimiter).astype('int32')\n",
    "    data = data[(-data[:,3]).argsort()]\n",
    "\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "    \n",
    "    train, test = [], []\n",
    "    user_id_dict, item_id_dict = {}, {}\n",
    "\n",
    "    for i in range(n_r - 1, -1, -1):\n",
    "        user_id, item_id, rating, timestamp = data[i]\n",
    "        if i < int(test_size * n_r): # test set\n",
    "            if user_id in user_id_dict and item_id in item_id_dict:\n",
    "                test.append((user_id_dict[user_id], item_id_dict[item_id], rating, timestamp))\n",
    "        else: # training set\n",
    "            if user_id not in user_id_dict:\n",
    "                user_id_dict[user_id] = len(user_id_dict)\n",
    "            if item_id not in item_id_dict:\n",
    "                item_id_dict[item_id] = len(item_id_dict)\n",
    "            train.append((user_id_dict[user_id], item_id_dict[item_id], rating, timestamp))\n",
    "    \n",
    "    train, test = np.array(train), np.array(test)\n",
    "\n",
    "    n_u = np.unique(train[:,0]).size  # num of users\n",
    "    n_m = np.unique(train[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of ratings: {}'.format(n_r))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 751\n",
      "num of movies: 1616\n",
      "num of training ratings: 80000\n",
      "num of test ratings: 2863\n"
     ]
    }
   ],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = 'data'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "\n",
    "# Data Load\n",
    "try:\n",
    "    if dataset == 'ML-100K':\n",
    "        path = data_path + '/MovieLens_100K/'\n",
    "        n_m, n_u, train, test = load_data_100k(path=path, delimiter='\\t')\n",
    "\n",
    "    elif dataset == 'ML-1M':\n",
    "        path = data_path + '/MovieLens_1M/'\n",
    "        n_m, n_u, train, test = load_data_1m(path=path, delimiter='::')\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "except ValueError as e:\n",
    "    print('Error: Unable to load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_to_sessions(data: list[tuple[int, int, int, int]]) -> dict[tuple[int, int], list[list[int]]]:\n",
    "    user_sessions_dict = {}\n",
    "    for user_id, item_id, _, timestamp in data:\n",
    "        user_id, item_id, timestamp = int(user_id), int(item_id), int(timestamp)\n",
    "        if user_id not in user_sessions_dict:\n",
    "            user_sessions_dict[user_id] = []\n",
    "        if len(user_sessions_dict[user_id]) == 0 or timestamp - user_sessions_dict[user_id][-1][-1][1] > SESSION_TIME_GAP_SECONDS:\n",
    "            user_sessions_dict[user_id].append([])\n",
    "        user_sessions_dict[user_id][-1].append((item_id, timestamp))\n",
    "    for user_id, user_sessions in user_sessions_dict.items():\n",
    "        for user_session_index, user_session_items_ids in enumerate(user_sessions):\n",
    "            user_sessions_dict[user_id][user_session_index] = [item_id for item_id, _ in user_session_items_ids], [timestamp // DAY_SECONDS_LENGTH for _, timestamp in user_session_items_ids]\n",
    "    user_item_sessions_dict = {}\n",
    "    for user_id, user_sessions in user_sessions_dict.items():\n",
    "        for user_session_items_ids, user_session_items_daystamps in user_sessions:\n",
    "            for item_id in user_session_items_ids:\n",
    "                user_item_sessions_dict[(user_id, item_id)] = tuple(user_session_items_ids), tuple(user_session_items_daystamps)\n",
    "    return user_item_sessions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_dict = divide_to_sessions(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(tf.keras.Model):\n",
    "    def __init__(self, users_count, items_count, average_rating, current_day, latent_dim=LATENT_DIM, regularization_factor=REGULARIZATION):\n",
    "        super().__init__()\n",
    "        self.regularization_factor = regularization_factor\n",
    "        self.user_embeddings = Embedding(users_count, latent_dim,\n",
    "                                  embeddings_regularizer=l2(regularization_factor),\n",
    "                                  name=\"user_embedding\")\n",
    "        self.item_embeddings = Embedding(items_count, latent_dim,\n",
    "                                  embeddings_regularizer=l2(regularization_factor),\n",
    "                                  name=\"item_embedding\")\n",
    "        self.average_rating = average_rating\n",
    "        self.user_biases = Embedding(users_count, 1,\n",
    "                                   embeddings_regularizer=l2(regularization_factor),\n",
    "                                   name=\"user_bias\")\n",
    "        self.item_biases = Embedding(items_count, 1,\n",
    "                                   embeddings_regularizer=l2(regularization_factor),\n",
    "                                   name=\"item_bias\")\n",
    "        self.session_biases = Embedding(len(set([session_items_ids for session_items_ids, _ in sessions_dict.values()])), 1,\n",
    "                                      embeddings_regularizer=l2(regularization_factor),\n",
    "                                      name=\"session_bias\")\n",
    "        self.user_session_decaying_rates = Embedding(users_count, 1,\n",
    "                                   embeddings_regularizer=l2(regularization_factor),\n",
    "                                   name=\"user_session_decaying_rate\")\n",
    "        self.current_day = tf.constant(current_day, dtype=tf.float32)\n",
    "\n",
    "\n",
    "        print(\"finish init\")\n",
    "\n",
    "    def get_session_presentation(self, user_id, session_items_ids, session_items_daystamps):\n",
    "        user_embedding = self.user_embeddings(user_id)\n",
    "        \n",
    "        session_items_ids = tf.boolean_mask(session_items_ids, session_items_ids != -1)\n",
    "        session_items_daystamps = tf.boolean_mask(session_items_daystamps, session_items_daystamps != -1)\n",
    "        session_items_embeddings = self.item_embeddings(session_items_ids)\n",
    "        \n",
    "        session_items_predicts = tf.reduce_sum(user_embedding[:, None, :] * session_items_embeddings, axis=2)\n",
    "        user_decaying_rate = tf.squeeze(self.user_session_decaying_rates(user_id))\n",
    "        user_decaying_rate = tf.nn.relu(user_decaying_rate)\n",
    "        session_items_decaying_factors = tf.exp(-tf.abs(session_items_daystamps - self.current_day)\n",
    "                                                 * user_decaying_rate\n",
    "                                                 )\n",
    "\n",
    "        session_items_scores = session_items_predicts * session_items_decaying_factors\n",
    "        return tf.reduce_mean(session_items_scores, axis=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_id, item_id, session_items_ids, session_items_daystamps = inputs\n",
    "\n",
    "        user_embedding = self.user_embeddings(user_id)\n",
    "        item_embedding = self.item_embeddings(item_id)\n",
    "        raw_prediction = tf.reduce_sum(user_embedding * item_embedding, axis=1)\n",
    "\n",
    "        session_predict = self.get_session_presentation(user_id, session_items_ids, session_items_daystamps)\n",
    "\n",
    "        total_bias = (\n",
    "            tf.squeeze(self.user_biases(user_id)) +\n",
    "            tf.squeeze(self.item_biases(item_id)) +\n",
    "            tf.squeeze(self.session_biases(session_predict))\n",
    "        )\n",
    "\n",
    "        return raw_prediction + self.average_rating + total_bias\n",
    "    \n",
    "    def l2_loss(self, y_true, y_predict, user_id, item_id, session_items_ids, session_items_daystamps):\n",
    "        squared_error = tf.square(y_true - y_predict)\n",
    "        user_embedding, item_embedding = self.user_embeddings(user_id), self.item_embeddings(item_id)\n",
    "        user_embedding_norm = tf.reduce_sum(tf.square(user_embedding), axis=1)\n",
    "        item_embedding_norm = tf.reduce_sum(tf.square(item_embedding), axis=1)\n",
    "        user_bias = tf.squeeze(self.user_biases(user_id))\n",
    "        item_bias = tf.squeeze(self.item_biases(item_id))\n",
    "        session_predict = self.get_session_presentation(user_id, session_items_ids, session_items_daystamps)\n",
    "        session_bias = tf.squeeze(self.session_biases(session_predict))\n",
    "        user_bias_norm = tf.square(user_bias)\n",
    "        item_bias_norm = tf.square(item_bias)\n",
    "        session_bias_norm = tf.square(session_bias)\n",
    "        session_decaying_rate = tf.squeeze(self.user_session_decaying_rates(user_id))\n",
    "        session_decaying_rate_norm = tf.square(session_decaying_rate)\n",
    "\n",
    "        regularized_loss = self.regularization_factor * (user_embedding_norm + item_embedding_norm + user_bias_norm + item_bias_norm + session_bias_norm + session_decaying_rate_norm)\n",
    "        return squared_error + regularized_loss\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        (user_id, item_id, session_items_ids, session_items_daystamps), y_true = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_predict = self((user_id, item_id, session_items_ids, session_items_daystamps), training=True)\n",
    "            loss = self.l2_loss(y_true, y_predict, user_id, item_id, session_items_ids, session_items_daystamps)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users, train_items, train_ratings = train[:,0], train[:,1], np.float32(train[:,2])\n",
    "train_sessions_items = [sessions_dict[(row[0], row[1])][0][: sessions_dict[(row[0], row[1])][0].index(row[1]) + 1] for row in train]\n",
    "train_sessions_daystamps = [sessions_dict[(row[0], row[1])][1][: sessions_dict[(row[0], row[1])][0].index(row[1]) + 1] for row in train]\n",
    "max_session_length = max(len(session_items) for session_items in train_sessions_items)\n",
    "train_sessions_items = np.array([session_items + (-1, ) * (max_session_length - len(session_items)) for session_items in train_sessions_items])\n",
    "train_sessions_daystamps = np.array([session_daystamps + (-1, ) * (max_session_length - len(session_daystamps)) for session_daystamps in train_sessions_daystamps], dtype=np.float32)\n",
    "test_users, test_items, test_ratings = test[:,0], test[:,1], np.float32(test[:,2])\n",
    "\n",
    "train_average_rating = np.mean(train_ratings)\n",
    "test_average_timestamp = np.mean(test[:,3])\n",
    "test_average_daystamp = int(test_average_timestamp // DAY_SECONDS_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish init\n",
      "Epoch 1/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1ms/step - loss: 1.1106\n",
      "Epoch 2/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1ms/step - loss: 0.9833\n",
      "Epoch 3/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1ms/step - loss: 0.9420\n",
      "Epoch 4/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 915us/step - loss: 0.9201\n",
      "Epoch 5/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 912us/step - loss: 0.9069\n",
      "Epoch 6/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 914us/step - loss: 0.8976\n",
      "Epoch 7/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 903us/step - loss: 0.8908\n",
      "Epoch 8/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 966us/step - loss: 0.8857\n",
      "Epoch 9/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 903us/step - loss: 0.8819\n",
      "Epoch 10/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 968us/step - loss: 0.8783\n",
      "Epoch 11/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1ms/step - loss: 0.8756\n",
      "Epoch 12/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.8731\n",
      "Epoch 13/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.8715\n",
      "Epoch 14/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1ms/step - loss: 0.8697\n",
      "Epoch 15/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1ms/step - loss: 0.8681\n",
      "Epoch 16/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1ms/step - loss: 0.8668\n",
      "Epoch 17/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1ms/step - loss: 0.8653\n",
      "Epoch 18/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 941us/step - loss: 0.8643\n",
      "Epoch 19/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 984us/step - loss: 0.8629\n",
      "Epoch 20/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1ms/step - loss: 0.8617\n",
      "Epoch 21/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 973us/step - loss: 0.8608\n",
      "Epoch 22/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 879us/step - loss: 0.8596\n",
      "Epoch 23/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 880us/step - loss: 0.8581\n",
      "Epoch 24/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 909us/step - loss: 0.8569\n",
      "Epoch 25/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 967us/step - loss: 0.8554\n",
      "Epoch 26/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 972us/step - loss: 0.8536\n",
      "Epoch 27/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 917us/step - loss: 0.8518\n",
      "Epoch 28/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 935us/step - loss: 0.8498\n",
      "Epoch 29/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 983us/step - loss: 0.8477\n",
      "Epoch 30/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1ms/step - loss: 0.8452\n",
      "Epoch 31/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.8424\n",
      "Epoch 32/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 990us/step - loss: 0.8397\n",
      "Epoch 33/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1ms/step - loss: 0.8363\n",
      "Epoch 34/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 1ms/step - loss: 0.8331\n",
      "Epoch 35/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 983us/step - loss: 0.8292\n",
      "Epoch 36/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 936us/step - loss: 0.8258\n",
      "Epoch 37/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 956us/step - loss: 0.8218\n",
      "Epoch 38/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 931us/step - loss: 0.8181\n",
      "Epoch 39/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 920us/step - loss: 0.8143\n",
      "Epoch 40/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 914us/step - loss: 0.8103\n",
      "Epoch 41/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 956us/step - loss: 0.8064\n",
      "Epoch 42/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 938us/step - loss: 0.8024\n",
      "Epoch 43/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 961us/step - loss: 0.7985\n",
      "Epoch 44/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 957us/step - loss: 0.7945\n",
      "Epoch 45/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1ms/step - loss: 0.7907\n",
      "Epoch 46/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 983us/step - loss: 0.7869\n",
      "Epoch 47/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1ms/step - loss: 0.7827\n",
      "Epoch 48/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 978us/step - loss: 0.7785\n",
      "Epoch 49/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 976us/step - loss: 0.7747\n",
      "Epoch 50/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 957us/step - loss: 0.7704\n",
      "Epoch 51/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 984us/step - loss: 0.7666\n",
      "Epoch 52/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 956us/step - loss: 0.7625\n",
      "Epoch 53/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 903us/step - loss: 0.7582\n",
      "Epoch 54/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 904us/step - loss: 0.7543\n",
      "Epoch 55/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.7496\n",
      "Epoch 56/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1ms/step - loss: 0.7459\n",
      "Epoch 57/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1ms/step - loss: 0.7415\n",
      "Epoch 58/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.7376\n",
      "Epoch 59/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1ms/step - loss: 0.7333\n",
      "Epoch 60/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 989us/step - loss: 0.7294\n",
      "Epoch 61/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 997us/step - loss: 0.7253\n",
      "Epoch 62/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 950us/step - loss: 0.7212\n",
      "Epoch 63/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 981us/step - loss: 0.7173\n",
      "Epoch 64/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 967us/step - loss: 0.7134\n",
      "Epoch 65/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 953us/step - loss: 0.7095\n",
      "Epoch 66/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 990us/step - loss: 0.7058\n",
      "Epoch 67/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1ms/step - loss: 0.7022\n",
      "Epoch 68/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1ms/step - loss: 0.6985\n",
      "Epoch 69/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 964us/step - loss: 0.6949\n",
      "Epoch 70/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 965us/step - loss: 0.6914\n",
      "Epoch 71/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 963us/step - loss: 0.6879\n",
      "Epoch 72/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 985us/step - loss: 0.6846\n",
      "Epoch 73/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 967us/step - loss: 0.6812\n",
      "Epoch 74/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 963us/step - loss: 0.6782\n",
      "Epoch 75/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 994us/step - loss: 0.6748\n",
      "Epoch 76/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 979us/step - loss: 0.6720\n",
      "Epoch 77/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 974us/step - loss: 0.6689\n",
      "Epoch 78/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1ms/step - loss: 0.6659\n",
      "Epoch 79/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 1ms/step - loss: 0.6630\n",
      "Epoch 80/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1ms/step - loss: 0.6603\n",
      "Epoch 81/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 1ms/step - loss: 0.6576\n",
      "Epoch 82/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 1ms/step - loss: 0.6550\n",
      "Epoch 83/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1ms/step - loss: 0.6523\n",
      "Epoch 84/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 1ms/step - loss: 0.6497\n",
      "Epoch 85/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 1ms/step - loss: 0.6475\n",
      "Epoch 86/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 1ms/step - loss: 0.6451\n",
      "Epoch 87/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1ms/step - loss: 0.6427\n",
      "Epoch 88/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1ms/step - loss: 0.6406\n",
      "Epoch 89/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1ms/step - loss: 0.6385\n",
      "Epoch 90/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1ms/step - loss: 0.6362\n",
      "Epoch 91/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1ms/step - loss: 0.6342\n",
      "Epoch 92/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1ms/step - loss: 0.6322\n",
      "Epoch 93/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1ms/step - loss: 0.6303\n",
      "Epoch 94/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1ms/step - loss: 0.6284\n",
      "Epoch 95/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1ms/step - loss: 0.6267\n",
      "Epoch 96/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1ms/step - loss: 0.6247\n",
      "Epoch 97/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1ms/step - loss: 0.6231\n",
      "Epoch 98/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1ms/step - loss: 0.6215\n",
      "Epoch 99/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 0.6198\n",
      "Epoch 100/100\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 1ms/step - loss: 0.6184\n"
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(n_u, n_m, train_average_rating, test_average_daystamp)\n",
    "model.compile(optimizer=SGD(learning_rate=LEARNING_RATE))\n",
    "history = model.fit(\n",
    "    [train_users, train_items, train_sessions_items, train_sessions_daystamps], train_ratings,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(users_matrix, items_matrix, user_biases, item_biases, _, _), average_rating = model.get_weights(), model.average_rating\n",
    "ratings_matrix = np.dot(users_matrix, items_matrix.T) + user_biases + item_biases.T + train_average_rating\n",
    "ratings_matrix = np.clip(ratings_matrix, 1, 5)\n",
    "\n",
    "# Save the model\n",
    "with open(path + f\"mf_prediction_{LATENT_DIM}_dims.pickle\", 'wb') as f:\n",
    "    pickle.dump(ratings_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.9772446155548096\n"
     ]
    }
   ],
   "source": [
    "# Get Test Score\n",
    "# test_ratings_predicted = np.clip(model.predict([test_users, test_items]), 1, 5) # predict using the model\n",
    "test_ratings_predicted = np.array([ratings_matrix[test_user, test_item] for test_user, test_item in zip(test_users, test_items)]) # predict using the matrix\n",
    "\n",
    "# check test rmse\n",
    "try:\n",
    "    test_rmse = root_mean_squared_error(test_ratings, test_ratings_predicted)\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
