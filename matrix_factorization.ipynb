{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset among 'ML-100K' and 'ML-1M'\n",
    "dataset = 'ML-100K'\n",
    "ML_1M_TEST_SIZE = 0.1\n",
    "\n",
    "# Model hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.002\n",
    "REGULARIZATION = 0.05\n",
    "EPOCHS = 10 # TODO - Increase this value to 100\n",
    "\n",
    "# Matrix factorization hyperparameters\n",
    "LATENT_DIM = 25 # Concepts count\n",
    "\n",
    "# Session\n",
    "SESSION_TIME_GAP_SEC = 604800 # 7 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "    test_size = len(test)\n",
    "    total = total[total[:,3].argsort()] # Sort by timestamp\n",
    "    \n",
    "    train = total[:-test_size]\n",
    "    test = total[-test_size:]\n",
    "\n",
    "    train_users, train_items = set(train[:, 0]), set(train[:, 1])\n",
    "    test = test[[(test_record[0] in train_users and test_record[1] in train_items) for test_record in test]]\n",
    "\n",
    "    n_u = np.unique(train[:,0]).size  # num of users\n",
    "    n_m = np.unique(train[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    user_id_dict = {}\n",
    "    for i, user_id in enumerate(np.unique(train[:,0]).tolist()):\n",
    "        user_id_dict[user_id] = i\n",
    "    \n",
    "    item_id_dict = {}\n",
    "    for i, item_id in enumerate(np.unique(train[:,1]).tolist()):\n",
    "        item_id_dict[item_id] = i\n",
    "\n",
    "    train = np.array([(user_id_dict[record[0]], item_id_dict[record[1]], record[2], record[3]) for record in train])\n",
    "    test = np.array([(user_id_dict[record[0]], item_id_dict[record[1]], record[2], record[3]) for record in test])\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, test\n",
    "\n",
    "def load_data_1m(path='./', delimiter='::', test_size=ML_1M_TEST_SIZE):\n",
    "    data = np.genfromtxt(path+'movielens_1m_dataset.dat', skip_header=0, delimiter=delimiter).astype('int32')\n",
    "    data = data[(-data[:,3]).argsort()]\n",
    "\n",
    "    n_u = np.unique(data[:,0]).size  # num of users\n",
    "    n_m = np.unique(data[:,1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    user_dict = {}\n",
    "    for i, user_id in enumerate(np.unique(data[:,0]).tolist()):\n",
    "        user_dict[user_id] = i\n",
    "    item_dict = {}\n",
    "    for i, item_id in enumerate(np.unique(data[:,1]).tolist()):\n",
    "        item_dict[item_id] = i\n",
    "\n",
    "    idx = np.arange(n_r)\n",
    "    \n",
    "    train, test = [], []\n",
    "    training_users = set()\n",
    "    \n",
    "    for i in range(n_r - 1, -1, -1):\n",
    "        user_id = user_dict[data[idx[i], 0]]\n",
    "        item_id = item_dict[data[idx[i], 1]]\n",
    "        rating = data[idx[i], 2]\n",
    "        timestamp = data[idx[i], 3]\n",
    "        if i < int(test_size * n_r): # test set\n",
    "            if user_id not in training_users:\n",
    "                continue\n",
    "            test.append((user_id - 1, item_id, rating, timestamp))\n",
    "        else: # training set\n",
    "            training_users.add(user_id)\n",
    "            train.append((user_id, item_id, rating, timestamp))\n",
    "\n",
    "    train, test = np.array(train), np.array(test)\n",
    "\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of ratings: {}'.format(n_r))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 751\n",
      "num of movies: 1616\n",
      "num of training ratings: 80000\n",
      "num of test ratings: 2863\n"
     ]
    }
   ],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = 'data'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "\n",
    "# Data Load\n",
    "try:\n",
    "    if dataset == 'ML-100K':\n",
    "        path = data_path + '/MovieLens_100K/'\n",
    "        n_m, n_u, train, test = load_data_100k(path=path, delimiter='\\t')\n",
    "\n",
    "    elif dataset == 'ML-1M':\n",
    "        path = data_path + '/MovieLens_1M/'\n",
    "        n_m, n_u, train, test = load_data_1m(path=path, delimiter='::')\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "except ValueError as e:\n",
    "    print('Error: Unable to load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_to_sessions(data: list[tuple[int, int, int, int]]) -> dict[tuple[int, int], list[list[int]]]:\n",
    "    user_sessions_dict = {}\n",
    "    for user_id, item_id, _, timestamp in data:\n",
    "        user_id, item_id, timestamp = int(user_id), int(item_id), int(timestamp)\n",
    "        if user_id not in user_sessions_dict:\n",
    "            user_sessions_dict[user_id] = []\n",
    "        if len(user_sessions_dict[user_id]) == 0 or timestamp - user_sessions_dict[user_id][-1][-1][1] > SESSION_TIME_GAP_SEC:\n",
    "            user_sessions_dict[user_id].append([])\n",
    "        user_sessions_dict[user_id][-1].append((item_id, timestamp))\n",
    "    for user_id, user_sessions in user_sessions_dict.items():\n",
    "        for i, user_session in enumerate(user_sessions):\n",
    "            user_sessions_dict[user_id][i] = [item_id for item_id, _ in user_session]\n",
    "    user_item_sessions_dict = {}\n",
    "    for user_id, user_sessions in user_sessions_dict.items():\n",
    "        for user_session in user_sessions:\n",
    "            for item_id in user_session:\n",
    "                user_item_sessions_dict[(user_id, item_id)] = tuple(user_session)\n",
    "    return user_item_sessions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_dict = divide_to_sessions(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, avg_ratings, latent_dim=LATENT_DIM, reg=REGULARIZATION):\n",
    "        super().__init__()\n",
    "        self.reg = reg\n",
    "        self.user_emb = Embedding(num_users, latent_dim,\n",
    "                                  embeddings_regularizer=l2(reg),\n",
    "                                  name=\"user_embedding\")\n",
    "        self.item_emb = Embedding(num_items, latent_dim,\n",
    "                                  embeddings_regularizer=l2(reg),\n",
    "                                  name=\"item_embedding\")\n",
    "        self.avg_rating = avg_ratings\n",
    "        self.user_bias = Embedding(num_users, 1,\n",
    "                                   embeddings_regularizer=l2(reg),\n",
    "                                   name=\"user_bias\")\n",
    "        self.item_bias = Embedding(num_items, 1,\n",
    "                                   embeddings_regularizer=l2(reg),\n",
    "                                   name=\"item_bias\")\n",
    "        self.session_bias = Embedding(len(set(sessions_dict.values())), 1,\n",
    "                                      embeddings_regularizer=l2(reg),\n",
    "                                      name=\"session_bias\")\n",
    "        print(\"finish init\")\n",
    "\n",
    "    def get_session_predict(self, user_vec, session_items):\n",
    "        session_items = tf.boolean_mask(session_items, session_items != -1)\n",
    "        session_items_vecs = self.item_emb(session_items)\n",
    "        \n",
    "        session_items_scores = tf.reduce_sum(user_vec[:, None, :] * session_items_vecs, axis=2)\n",
    "\n",
    "        return tf.reduce_mean(session_items_scores, axis=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user, item, session_items = inputs\n",
    "        user_vec = self.user_emb(user)\n",
    "        item_vec = self.item_emb(item)\n",
    "        dot_product = tf.reduce_sum(user_vec * item_vec, axis=1)\n",
    "        session_predict = self.get_session_predict(user_vec, session_items)\n",
    "        bias = (\n",
    "            tf.squeeze(self.user_bias(user)) +\n",
    "            tf.squeeze(self.item_bias(item)) +\n",
    "            tf.squeeze(self.session_bias(session_predict))\n",
    "        )\n",
    "        return dot_product + self.avg_rating + bias\n",
    "    \n",
    "    def l2_loss(self, y_true, y_pred, user, item, session_items):\n",
    "        squared_error = tf.square(y_true - y_pred)\n",
    "        user_vec, item_vec = self.user_emb(user), self.item_emb(item)\n",
    "        user_vec_norm = tf.reduce_sum(tf.square(user_vec), axis=1)\n",
    "        item_vec_norm = tf.reduce_sum(tf.square(item_vec), axis=1)\n",
    "        user_bias = tf.squeeze(self.user_bias(user))\n",
    "        item_bias = tf.squeeze(self.item_bias(item))\n",
    "        session_bias = tf.squeeze(self.session_bias(self.get_session_predict(user_vec, session_items)))\n",
    "        user_bias_norm = tf.square(user_bias)\n",
    "        item_bias_norm = tf.square(item_bias)\n",
    "        session_norm = tf.square(session_bias)\n",
    "\n",
    "        reg_loss = self.reg * (user_vec_norm + item_vec_norm + user_bias_norm + item_bias_norm + session_norm)\n",
    "        return squared_error + reg_loss\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        (user, item, session_items), y_true = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self((user, item, session_items), training=True)\n",
    "            loss = self.l2_loss(y_true, y_pred, user, item, session_items)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users, train_items, train_ratings = train[:,0], train[:,1], np.float32(train[:,2])\n",
    "train_sessions_items = [sessions_dict[(row[0], row[1])] for row in train]\n",
    "max_session_length = max(len(session_items) for session_items in train_sessions_items)\n",
    "train_sessions_items = np.array([session_items + (-1, ) * (max_session_length - len(session_items)) for session_items in train_sessions_items])\n",
    "test_users, test_items, test_ratings = test[:,0], test[:,1], np.float32(test[:,2])\n",
    "\n",
    "train_avg_rating = np.mean(train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1  2  3 -1], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "example_session_items = np.array([1,2,3, -1])\n",
    "example_session_tensor = tf.constant(example_session_items, dtype=tf.int32)\n",
    "example_tensor_no_negative = tf.boolean_mask(example_session_tensor, example_session_tensor != 0)\n",
    "print(example_tensor_no_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish init\n",
      "Epoch 1/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 881us/step - loss: 1.1101\n",
      "Epoch 2/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 925us/step - loss: 0.9832\n",
      "Epoch 3/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 885us/step - loss: 0.9419\n",
      "Epoch 4/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 925us/step - loss: 0.9207\n",
      "Epoch 5/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 900us/step - loss: 0.9072\n",
      "Epoch 6/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 905us/step - loss: 0.8979\n",
      "Epoch 7/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 935us/step - loss: 0.8911\n",
      "Epoch 8/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 952us/step - loss: 0.8861\n",
      "Epoch 9/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 947us/step - loss: 0.8818\n",
      "Epoch 10/10\n",
      "\u001b[1m80000/80000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 938us/step - loss: 0.8786\n"
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(n_u, n_m, train_avg_rating)\n",
    "model.compile(optimizer=SGD(learning_rate=LEARNING_RATE))\n",
    "history = model.fit(\n",
    "    [train_users, train_items, train_sessions_items], train_ratings,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrix, items_matrix, user_biases, item_biases, avg_rating = model.get_weights()\n",
    "ratings_matrix = np.dot(users_matrix, items_matrix.T) + user_biases + item_biases.T + train_avg_rating\n",
    "ratings_matrix = np.clip(ratings_matrix, 1, 5)\n",
    "\n",
    "# Save the model\n",
    "with open(path + \"mf_prediction.pickle\", 'wb') as f:\n",
    "    pickle.dump(ratings_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.0086641311645508\n"
     ]
    }
   ],
   "source": [
    "# Get Test Score\n",
    "# test_ratings_predicted = np.clip(model.predict([test_users, test_items]), 1, 5) # predict using the model\n",
    "test_ratings_predicted = np.array([ratings_matrix[test_user, test_item] for test_user, test_item in zip(test_users, test_items)]) # predict using the matrix\n",
    "\n",
    "\n",
    "# check test rmse\n",
    "test_rmse = root_mean_squared_error(test_ratings, test_ratings_predicted)\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
