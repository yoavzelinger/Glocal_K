{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset among 'ML-100K' and 'ML-1M'\n",
    "dataset = 'ML-100K'\n",
    "\n",
    "# Data split parameters\n",
    "VALIDATION_USERS_RATINGS_COUNT = 4\n",
    "TEST_USERS_RATINGS_COUNT = 6\n",
    "\n",
    "# Model hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.002\n",
    "REGULARIZATION = 0.05\n",
    "EPOCHS = 10 # TODO - Increase this value to 100\n",
    "\n",
    "# Matrix factorization hyperparameters\n",
    "LATENT_DIM = 25 # Concepts count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "    total = total[total[:,3].argsort()] # Sort by timestamp\n",
    "    \n",
    "    users_ratings_counts = {}\n",
    "    train, validation, test = [], [], []\n",
    "    for user_id, item_id, rating, _ in total[:: -1]:\n",
    "        users_ratings_counts[user_id] = users_ratings_counts.get(user_id, 0) + 1\n",
    "        if users_ratings_counts[user_id] <= TEST_USERS_RATINGS_COUNT:\n",
    "            test.append((user_id - 1, item_id - 1, rating))\n",
    "        elif users_ratings_counts[user_id] <= VALIDATION_USERS_RATINGS_COUNT + TEST_USERS_RATINGS_COUNT:\n",
    "            validation.append((user_id - 1, item_id - 1, rating))\n",
    "        else:\n",
    "            train.append((user_id - 1, item_id - 1, rating))\n",
    "    train, validation, test = np.array(train), np.array(validation), np.array(test)\n",
    "\n",
    "    n_u = np.unique(total[:,0]).size  # num of users\n",
    "    n_m = np.unique(total[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_validation = validation.shape[0]  # num of validation ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of validation ratings: {}'.format(n_validation))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, validation, test\n",
    "\n",
    "def load_data_1m(path='./', delimiter='::'):\n",
    "    data = np.genfromtxt(path+'movielens_1m_dataset.dat', skip_header=0, delimiter=delimiter).astype('int32')\n",
    "    data = data[(-data[:,3]).argsort()]\n",
    "\n",
    "    n_u = np.unique(data[:,0]).size  # num of users\n",
    "    n_m = np.unique(data[:,1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    user_dict = {}\n",
    "    for i, user_id in enumerate(np.unique(data[:,0]).tolist()):\n",
    "        user_dict[user_id] = i\n",
    "    item_dict = {}\n",
    "    for i, item_id in enumerate(np.unique(data[:,1]).tolist()):\n",
    "        item_dict[item_id] = i\n",
    "\n",
    "    idx = np.arange(n_r)\n",
    "\n",
    "    users_ratings_counts = {}\n",
    "    train, validation, test = [], [], []\n",
    "    for i in range(n_r):\n",
    "        user_id = user_dict[data[idx[i], 0]]\n",
    "        item_id = item_dict[data[idx[i], 1]]\n",
    "        rating = data[idx[i], 2]\n",
    "        users_ratings_counts[user_id] = users_ratings_counts.get(user_id, 0) + 1\n",
    "        if users_ratings_counts[user_id] <= TEST_USERS_RATINGS_COUNT:\n",
    "            test.append((user_id - 1, item_id - 1, rating))\n",
    "        elif users_ratings_counts[user_id] <= VALIDATION_USERS_RATINGS_COUNT + TEST_USERS_RATINGS_COUNT:\n",
    "            validation.append((user_id - 1, item_id - 1, rating))\n",
    "        else:\n",
    "            train.append((user_id - 1, item_id - 1, rating))\n",
    "\n",
    "    train, validation, test = np.array(train), np.array(validation), np.array(test)\n",
    "\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_validation = validation.shape[0]  # num of validation ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of ratings: {}'.format(n_r))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of validation ratings: {}'.format(n_validation))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, validation, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 943\n",
      "num of movies: 1682\n",
      "num of training ratings: 90570\n",
      "num of validation ratings: 3772\n",
      "num of test ratings: 5658\n"
     ]
    }
   ],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = 'data'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "\n",
    "# Data Load\n",
    "try:\n",
    "    if dataset == 'ML-100K':\n",
    "        path = data_path + '/MovieLens_100K/'\n",
    "        n_m, n_u, train, validation, test = load_data_100k(path=path, delimiter='\\t')\n",
    "\n",
    "    elif dataset == 'ML-1M':\n",
    "        path = data_path + '/MovieLens_1M/'\n",
    "        n_m, n_u, train, validation, test = load_data_1m(path=path, delimiter='::')\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "except ValueError as e:\n",
    "    print('Error: Unable to load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, avg_ratings, latent_dim=LATENT_DIM, reg=REGULARIZATION):\n",
    "        super().__init__()\n",
    "        self.user_emb = Embedding(num_users, latent_dim,\n",
    "                                  embeddings_regularizer=l2(reg),\n",
    "                                  name=\"user_embedding\")\n",
    "        self.item_emb = Embedding(num_items, latent_dim,\n",
    "                                  embeddings_regularizer=l2(reg),\n",
    "                                  name=\"item_embedding\")\n",
    "        self.user_bias = Embedding(num_users, 1,\n",
    "                                   embeddings_regularizer=l2(reg),\n",
    "                                   name=\"user_bias\")\n",
    "        self.item_bias = Embedding(num_items, 1,\n",
    "                                   embeddings_regularizer=l2(reg),\n",
    "                                   name=\"item_bias\")\n",
    "        self.avg_rating = avg_ratings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user, item = inputs\n",
    "        user_vec = self.user_emb(user)\n",
    "        item_vec = self.item_emb(item)\n",
    "        dot_product = tf.reduce_sum(user_vec * item_vec, axis=1)\n",
    "\n",
    "        bias = (\n",
    "            tf.squeeze(self.user_bias(user)) +\n",
    "            tf.squeeze(self.item_bias(item))\n",
    "        )\n",
    "        return dot_product + self.avg_rating + bias\n",
    "    \n",
    "    def l2_loss(self, y_true, y_pred, user, item):\n",
    "        squared_error = tf.square(y_true - y_pred)\n",
    "        user_vec, item_vec = self.user_emb(user), self.item_emb(item)\n",
    "        user_vec_norm, item_vec_norm = tf.reduce_sum(tf.square(user_vec), axis=1), tf.reduce_sum(tf.square(item_vec), axis=1)\n",
    "        user_bias, item_bias = self.user_bias(user), self.item_bias(item)\n",
    "        user_bias_norm, item_bias_norm = tf.squeeze(tf.square(user_bias)), tf.squeeze(tf.square(item_bias))\n",
    "        reg_loss = REGULARIZATION * (user_vec_norm + item_vec_norm + user_bias_norm + item_bias_norm)\n",
    "        return squared_error + reg_loss\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        (user, item), y_true = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self((user, item), training=True)\n",
    "            loss = self.l2_loss(y_true, y_pred, user, item)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users, train_items, train_ratings = train[:,0], train[:,1], np.float32(train[:,2])\n",
    "validation_users, validation_items, validation_ratings = validation[:,0], validation[:,1], np.float32(validation[:,2])\n",
    "test_users, test_items, test_ratings = test[:,0], test[:,1], np.float32(test[:,2])\n",
    "\n",
    "train_avg_rating = np.mean(train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 761us/step - loss: 1.0913\n",
      "Epoch 2/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 767us/step - loss: 0.9703\n",
      "Epoch 3/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 786us/step - loss: 0.9318\n",
      "Epoch 4/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 760us/step - loss: 0.9118\n",
      "Epoch 5/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 761us/step - loss: 0.8992\n",
      "Epoch 6/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 793us/step - loss: 0.8904\n",
      "Epoch 7/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 774us/step - loss: 0.8838\n",
      "Epoch 8/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 790us/step - loss: 0.8788\n",
      "Epoch 9/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 790us/step - loss: 0.8747\n",
      "Epoch 10/10\n",
      "\u001b[1m90570/90570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 808us/step - loss: 0.8713\n"
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(n_u, n_m, train_avg_rating)\n",
    "model.compile(optimizer=SGD(learning_rate=LEARNING_RATE))\n",
    "history = model.fit(\n",
    "    [train_users, train_items], train_ratings,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    # validation_data=([validation_users, validation_items], validation_ratings),\n",
    "    # callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Test RMSE: 1.0371140241622925\n"
     ]
    }
   ],
   "source": [
    "test_ratings_predicted = model.predict([test_users, test_items])\n",
    "test_ratings_predicted = np.clip(test_ratings_predicted, 1, 5)\n",
    "\n",
    "# check test rmse\n",
    "test_rmse = root_mean_squared_error(test_ratings, test_ratings_predicted)\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrix, items_matrix, user_biases, item_biases = model.get_weights()\n",
    "ratings_matrix = np.dot(users_matrix, items_matrix.T) + user_biases + item_biases.T + train_avg_rating\n",
    "ratings_matrix = np.clip(ratings_matrix, 1, 5)\n",
    "\n",
    "# Save the model\n",
    "with open(path + \"mf_prediction.pickle\", 'wb') as f:\n",
    "    pickle.dump(ratings_matrix, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
