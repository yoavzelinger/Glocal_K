{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset among 'ML-100K' and 'ML-1M'\n",
    "dataset = 'ML-100K'\n",
    "\n",
    "# Data split parameters\n",
    "VALIDATION_USERS_RATINGS_COUNT = 4\n",
    "TEST_USERS_RATINGS_COUNT = 6\n",
    "\n",
    "# Model hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.002\n",
    "REGULARIZATION = 0.05\n",
    "EPOCHS = 10 # TODO - Increase this value to 100\n",
    "\n",
    "# Matrix factorization hyperparameters\n",
    "LATENT_DIM = 25 # Concepts count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "    total = total[total[:,3].argsort()] # Sort by timestamp\n",
    "    \n",
    "    users_ratings_counts = {}\n",
    "    train, validation, test = [], [], []\n",
    "    for user_id, item_id, rating, _ in total[:: -1]:\n",
    "        users_ratings_counts[user_id] = users_ratings_counts.get(user_id, 0) + 1\n",
    "        if users_ratings_counts[user_id] <= TEST_USERS_RATINGS_COUNT:\n",
    "            test.append((user_id - 1, item_id - 1, rating))\n",
    "        elif users_ratings_counts[user_id] <= VALIDATION_USERS_RATINGS_COUNT + TEST_USERS_RATINGS_COUNT:\n",
    "            validation.append((user_id - 1, item_id - 1, rating))\n",
    "        else:\n",
    "            train.append((user_id - 1, item_id - 1, rating))\n",
    "    train, validation, test = np.array(train), np.array(validation), np.array(test)\n",
    "\n",
    "    n_u = np.unique(total[:,0]).size  # num of users\n",
    "    n_m = np.unique(total[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_validation = validation.shape[0]  # num of validation ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of validation ratings: {}'.format(n_validation))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, validation, test\n",
    "\n",
    "def load_data_1m(path='./', delimiter='::'):\n",
    "    data = np.genfromtxt(path+'movielens_1m_dataset.dat', skip_header=0, delimiter=delimiter).astype('int32')\n",
    "    data = data[(-data[:,3]).argsort()]\n",
    "\n",
    "    n_u = np.unique(data[:,0]).size  # num of users\n",
    "    n_m = np.unique(data[:,1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    user_dict = {}\n",
    "    for i, user_id in enumerate(np.unique(data[:,0]).tolist()):\n",
    "        user_dict[user_id] = i\n",
    "    item_dict = {}\n",
    "    for i, item_id in enumerate(np.unique(data[:,1]).tolist()):\n",
    "        item_dict[item_id] = i\n",
    "\n",
    "    idx = np.arange(n_r)\n",
    "\n",
    "    users_ratings_counts = {}\n",
    "    train, validation, test = [], [], []\n",
    "    for i in range(n_r):\n",
    "        user_id = user_dict[data[idx[i], 0]]\n",
    "        item_id = item_dict[data[idx[i], 1]]\n",
    "        rating = data[idx[i], 2]\n",
    "        users_ratings_counts[user_id] = users_ratings_counts.get(user_id, 0) + 1\n",
    "        if users_ratings_counts[user_id] <= TEST_USERS_RATINGS_COUNT:\n",
    "            test.append((user_id - 1, item_id - 1, rating))\n",
    "        elif users_ratings_counts[user_id] <= VALIDATION_USERS_RATINGS_COUNT + TEST_USERS_RATINGS_COUNT:\n",
    "            validation.append((user_id - 1, item_id - 1, rating))\n",
    "        else:\n",
    "            train.append((user_id - 1, item_id - 1, rating))\n",
    "\n",
    "    train, validation, test = np.array(train), np.array(validation), np.array(test)\n",
    "\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_validation = validation.shape[0]  # num of validation ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of ratings: {}'.format(n_r))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of validation ratings: {}'.format(n_validation))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train, validation, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 943\n",
      "num of movies: 1682\n",
      "num of training ratings: 90570\n",
      "num of validation ratings: 3772\n",
      "num of test ratings: 5658\n"
     ]
    }
   ],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = 'data'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "\n",
    "# Data Load\n",
    "try:\n",
    "    if dataset == 'ML-100K':\n",
    "        path = data_path + '/MovieLens_100K/'\n",
    "        n_m, n_u, train, validation, test = load_data_100k(path=path, delimiter='\\t')\n",
    "\n",
    "    elif dataset == 'ML-1M':\n",
    "        path = data_path + '/MovieLens_1M/'\n",
    "        n_m, n_u, train, validation, test = load_data_1m(path=path, delimiter='::')\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "except ValueError as e:\n",
    "    print('Error: Unable to load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, avg_ratings, latent_dim=LATENT_DIM, reg=REGULARIZATION):\n",
    "        super().__init__()\n",
    "        self.user_emb = Embedding(num_users, latent_dim,\n",
    "                                  embeddings_regularizer=l2(reg),\n",
    "                                  name=\"user_embedding\")\n",
    "        self.item_emb = Embedding(num_items, latent_dim,\n",
    "                                  embeddings_regularizer=l2(reg),\n",
    "                                  name=\"item_embedding\")\n",
    "        self.user_bias = Embedding(num_users, 1,\n",
    "                                   embeddings_regularizer=l2(reg),\n",
    "                                   name=\"user_bias\")\n",
    "        self.item_bias = Embedding(num_items, 1,\n",
    "                                   embeddings_regularizer=l2(reg),\n",
    "                                   name=\"item_bias\")\n",
    "        self.avg_rating = avg_ratings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user, item = inputs\n",
    "        user_vec = self.user_emb(user)\n",
    "        item_vec = self.item_emb(item)\n",
    "        dot_product = tf.reduce_sum(user_vec * item_vec, axis=1)\n",
    "\n",
    "        bias = (\n",
    "            tf.squeeze(self.user_bias(user)) +\n",
    "            tf.squeeze(self.item_bias(item))\n",
    "        )\n",
    "        return dot_product + self.avg_rating + bias\n",
    "    \n",
    "    def l2_loss(self, y_true, y_pred, user, item):\n",
    "        squared_error = tf.square(y_true - y_pred)\n",
    "        user_vec, item_vec = self.user_emb(user), self.item_emb(item)\n",
    "        user_vec_norm, item_vec_norm = tf.reduce_sum(tf.square(user_vec), axis=1), tf.reduce_sum(tf.square(item_vec), axis=1)\n",
    "        user_bias, item_bias = self.user_bias(user), self.item_bias(item)\n",
    "        user_bias_norm, item_bias_norm = tf.squeeze(tf.square(user_bias)), tf.squeeze(tf.square(item_bias))\n",
    "        reg_loss = REGULARIZATION * (user_vec_norm + item_vec_norm + user_bias_norm + item_bias_norm)\n",
    "        return squared_error + reg_loss\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        (user, item), y_true = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self((user, item), training=True)\n",
    "            loss = self.l2_loss(y_true, y_pred, user, item)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users, train_items, train_ratings = train[:,0], train[:,1], np.float32(train[:,2])\n",
    "validation_users, validation_items, validation_ratings = validation[:,0], validation[:,1], np.float32(validation[:,2])\n",
    "test_users, test_items, test_ratings = test[:,0], test[:,1], np.float32(test[:,2])\n",
    "\n",
    "train_avg_rating = np.mean(train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 4486/90570\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 822us/step - loss: 1.2276"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MatrixFactorization(n_u, n_m, train_avg_rating)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39mLEARNING_RATE))\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      4\u001b[0m     [train_users, train_items], train_ratings,\n\u001b[0;32m      5\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m      6\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# validation_data=([validation_users, validation_items], validation_ratings),\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:217\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    214\u001b[0m     iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    215\u001b[0m ):\n\u001b[0;32m    216\u001b[0m     opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mget_value()\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\optional_ops.py:176\u001b[0m, in \u001b[0;36m_OptionalImpl.has_value\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gen_optional_ops\u001b[38;5;241m.\u001b[39moptional_has_value(\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor, name\u001b[38;5;241m=\u001b[39mname\n\u001b[0;32m    178\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\t-yzelinger\\AppData\\Local\\anaconda3\\envs\\school_venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_optional_ops.py:172\u001b[0m, in \u001b[0;36moptional_has_value\u001b[1;34m(optional, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    171\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m    173\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptionalHasValue\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, optional)\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(n_u, n_m, train_avg_rating)\n",
    "model.compile(optimizer=SGD(learning_rate=LEARNING_RATE))\n",
    "history = model.fit(\n",
    "    [train_users, train_items], train_ratings,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    # validation_data=([validation_users, validation_items], validation_ratings),\n",
    "    # callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step\n",
      "Test RMSE: 1.1923863887786865\n"
     ]
    }
   ],
   "source": [
    "test_ratings_predicted = model.predict([test_users, test_items])\n",
    "test_ratings_predicted = np.clip(test_ratings_predicted, 1, 5)\n",
    "\n",
    "# check test rmse\n",
    "test_rmse = root_mean_squared_error(test_ratings, test_ratings_predicted)\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrix, items_matrix, user_biases, item_biases = model.get_weights()\n",
    "ratings_matrix = np.dot(users_matrix, items_matrix.T) + user_biases + item_biases.T + train_avg_rating\n",
    "ratings_matrix = np.clip(ratings_matrix, 1, 5)\n",
    "\n",
    "# Save the model\n",
    "with open(path + \"mf_prediction.pickle\", 'wb') as f:\n",
    "    pickle.dump(ratings_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id, item_id, rating = train_users[10], train_items[10], train_ratings[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.4805355], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([np.array([user_id]), np.array([item_id])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.4805355], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_vec = model.user_emb(np.array([user_id]))\n",
    "item_vec = model.item_emb(np.array([item_id]))\n",
    "dot_product = tf.reduce_sum(user_vec * item_vec, axis=1)\n",
    "bias = (\n",
    "            tf.squeeze(model.user_bias(user_id)) +\n",
    "            tf.squeeze(model.item_bias(item_id))\n",
    "        )\n",
    "dot_product + model.avg_rating + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.1569853e-12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.4805355], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_vec, item_vec = np.array(user_vec), np.array(item_vec)\n",
    "dot_product = np.sum(user_vec * item_vec, axis=1)\n",
    "print(dot_product)\n",
    "bias = (\n",
    "            tf.squeeze(model.user_bias(user_id)) +\n",
    "            tf.squeeze(model.item_bias(item_id))\n",
    "        )\n",
    "bias = np.array(bias)\n",
    "dot_product + model.avg_rating + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TensorShape([943, 25]),\n",
       " TensorShape([1682, 25]),\n",
       " TensorShape([943, 1]),\n",
       " TensorShape([1682, 1])]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.user_emb()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
